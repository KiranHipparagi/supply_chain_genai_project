"""
Build Planalytics Knowledge Graph in Azure Cosmos DB (Gremlin API)
"""
import os
import sys
import pandas as pd
import re
import time
import random
from pathlib import Path
from dotenv import load_dotenv
from gremlin_python.driver import client, serializer
import psycopg2
from sqlalchemy import create_engine

load_dotenv()

# CONFIGURATION
SAMPLE_SIZE = None  # Set to None for all data, or number for testing (e.g., 100)
BATCH_SIZE = 50  # Process records in batches
MAX_RETRIES = 5  # Retry attempts for rate limits


class AsyncPlanalyticsGremlinBuilder:
    def __init__(self):
        # Cosmos DB connection
        self.endpoint = os.getenv("COSMOS_ENDPOINT")
        self.cosmos_key = os.getenv("COSMOS_KEY")
        self.cosmos_database = os.getenv("COSMOS_DATABASE", "planalytics")
        self.cosmos_graph = os.getenv("COSMOS_GRAPH", "planalytics_graph")
        self.cosmos_port = int(os.getenv("COSMOS_PORT", "443"))
        
        if not all([self.endpoint, self.cosmos_key]):
            raise ValueError("Missing Cosmos DB environment variables (COSMOS_ENDPOINT, COSMOS_KEY)!")
        
        # Initialize Gremlin client
        self.gremlin_client = client.Client(
            f'wss://{self.endpoint}:{self.cosmos_port}/gremlin',
            'g',
            username=f"/dbs/{self.cosmos_database}/colls/{self.cosmos_graph}",
            password=self.cosmos_key,
            message_serializer=serializer.GraphSONSerializersV2d0()
        )
        
        # PostgreSQL connection
        pg_config = {
            'host': os.getenv('POSTGRES_HOST', 'localhost'),
            'database': 'planalytics_database',
            'user': os.getenv('POSTGRES_USER', 'postgres'),
            'password': os.getenv('POSTGRES_PASSWORD'),
            'port': os.getenv('POSTGRES_PORT', '5432')
        }
        
        connection_string = f"postgresql://{pg_config['user']}:{pg_config['password']}@{pg_config['host']}:{pg_config['port']}/{pg_config['database']}"
        self.pg_engine = create_engine(connection_string)
        
        print(f"‚úÖ Connected to Cosmos DB: {self.cosmos_endpoint}")
        print(f"‚úÖ Connected to PostgreSQL: planalytics_database")
    
    def close(self):
        """Close connections"""
        if self.gremlin_client:
            self.gremlin_client.close()
        if self.pg_engine:
            self.pg_engine.dispose()
    
    def escape_string(self, s):
        """Escape special characters for Gremlin queries"""
        if pd.isna(s) or s is None:
            return ""
        s = str(s)
        s = s.replace("'", "")  # Remove quotes
        s = re.sub(r'[^\w\s\-_.]', '', s)  # Keep alphanumeric, space, dash, underscore, dot
        return s.strip()
    
    def safe_string_for_id(self, s):
        """Create a safe string for use as vertex ID"""
        if pd.isna(s) or s is None:
            return ""
        s = str(s)
        s = re.sub(r'[^\w\-_.]', '_', s)  # Replace non-alphanumeric with underscore
        s = re.sub(r'_+', '_', s)  # Collapse multiple underscores
        return s.strip('_')
    
    def submit_query(self, query, max_retries=MAX_RETRIES):
        """Submit a Gremlin query with retry logic"""
        retry_count = 0
        base_delay = 1
        
        while retry_count < max_retries:
            try:
                result = self.gremlin_client.submit(query).all().result()
                return result
            except Exception as e:
                error_msg = str(e)
                if "429" in error_msg or "3200" in error_msg or "TooManyRequests" in error_msg:
                    retry_count += 1
                    delay = base_delay * (2 ** retry_count) + (random.random() * 0.5)
                    if retry_count == 1:
                        print(f"\n   ‚ö†Ô∏è  Rate limit hit, applying backoff...", end="")
                    time.sleep(delay)
                else:
                    print(f"\n   ‚ùå Error: {error_msg[:100]}")
                    return None
        
        print(f"\n   ‚ùå Failed after {max_retries} retries")
        return None
    
    def submit_batch(self, queries, show_progress=False):
        """Submit multiple queries and return success count"""
        success = 0
        for i, query in enumerate(queries):
            if self.submit_query(query):
                success += 1
            if show_progress and (i + 1) % 10 == 0:
                print(".", end="", flush=True)
        return success
    
    def drop_graph(self):
        """Clear the graph"""
        print("\nüßπ Clearing existing graph...")
        self.submit_query("g.V().drop()")
        print("   ‚úÖ Graph cleared")
    
    def load_product_hierarchy_batch(self):
        """Load product hierarchy with batch processing"""
        print("\nüì¶ Loading Product Hierarchy...")
        
        df = pd.read_sql_table('product_hierarchy', self.pg_engine)
        if SAMPLE_SIZE:
            df = df.head(SAMPLE_SIZE)
        print(f"   Read {len(df)} products from database")
        
        # Deduplicate departments and categories
        unique_depts = df['dept'].dropna().unique()
        unique_cats = df['category'].dropna().unique()
        
        print(f"\n   Creating {len(unique_depts)} departments and {len(unique_cats)} categories...")
        
        # Batch create hierarchy vertices
        queries = []
        
        # Departments
        for dept in unique_depts:
            dept_clean = self.escape_string(dept)
            dept_id = self.safe_string_for_id(dept)
            queries.append(f"g.V().has('Department', 'id', '{dept_id}').fold().coalesce(unfold(), addV('Department').property('pk', 'Department').property('id', '{dept_id}').property('name', '{dept_clean}'))")
        
        # Categories
        for cat in unique_cats:
            cat_clean = self.escape_string(cat)
            cat_id = self.safe_string_for_id(cat)
            queries.append(f"g.V().has('Category', 'id', '{cat_id}').fold().coalesce(unfold(), addV('Category').property('pk', 'Category').property('id', '{cat_id}').property('name', '{cat_clean}'))")
        
        print(f"   Submitting {len(queries)} hierarchy vertices", end="")
        self.submit_batch(queries, show_progress=True)
        print()
        
        # Batch create Product vertices
        print(f"   Creating {len(df)} products", end="")
        product_queries = []
        for idx, row in df.iterrows():
            prod_id = f"P_{row['product_id']}"
            prod_name = self.escape_string(row['product'])
            
            product_queries.append(f"g.V().has('Product', 'id', '{prod_id}').fold().coalesce(unfold(), addV('Product').property('pk', 'Product').property('id', '{prod_id}').property('product_id', {row['product_id']}).property('name', '{prod_name}'))")
            
            if len(product_queries) >= BATCH_SIZE:
                self.submit_batch(product_queries)
                product_queries = []
                print(".", end="", flush=True)
        
        if product_queries:
            self.submit_batch(product_queries)
        print()
        
        # Batch create edges (only for products with dept/category)
        print(f"   Creating product relationships", end="")
        edge_queries = []
        df_with_hierarchy = df.dropna(subset=['dept', 'category'])
        for idx, row in df_with_hierarchy.iterrows():
            prod_id = f"P_{row['product_id']}"
            dept_id = self.safe_string_for_id(row['dept'])
            cat_id = self.safe_string_for_id(row['category'])
            
            edge_queries.extend([
                f"g.V().has('Product', 'id', '{prod_id}').as('p').V().has('Category', 'id', '{cat_id}').coalesce(__.inE('IN_CATEGORY').where(outV().as('p')), addE('IN_CATEGORY').from('p'))",
                f"g.V().has('Category', 'id', '{cat_id}').as('c').V().has('Department', 'id', '{dept_id}').coalesce(__.inE('IN_DEPARTMENT').where(outV().as('c')), addE('IN_DEPARTMENT').from('c'))"
            ])
            
            if len(edge_queries) >= BATCH_SIZE:
                self.submit_batch(edge_queries)
                edge_queries = []
                print(".", end="", flush=True)
        
        if edge_queries:
            self.submit_batch(edge_queries)
        print()
        
        print(f"   ‚úÖ Product hierarchy loaded")
    
    def load_perishable_batch(self):
        """Load perishable info and link to products"""
        print("\nü•¨ Loading Perishable Product Info...")
        
        df = pd.read_sql_table('perishable', self.pg_engine)
        if SAMPLE_SIZE:
            df = df.head(SAMPLE_SIZE)
        print(f"   Read {len(df)} perishable products from database")
        
        # Create PerishableInfo vertices
        queries = []
        for idx, row in df.iterrows():
            perish_id = f"PERISH_{row['id']}"
            prod_name = self.escape_string(row['product'])
            min_p = self.escape_string(str(row['min_period']) if not pd.isna(row['min_period']) else "")
            max_p = self.escape_string(str(row['max_period']) if not pd.isna(row['max_period']) else "")
            metric = self.escape_string(row['period_metric'] if not pd.isna(row['period_metric']) else "")
            stor = self.escape_string(row['storage'] if not pd.isna(row['storage']) else "")
            
            queries.append(f"g.V().has('PerishableInfo', 'id', '{perish_id}').fold().coalesce(unfold(), addV('PerishableInfo').property('pk', 'PerishableInfo').property('id', '{perish_id}').property('product', '{prod_name}').property('min_period', '{min_p}').property('max_period', '{max_p}').property('period_metric', '{metric}').property('storage', '{stor}'))")
        
        print(f"   Creating {len(queries)} perishable info vertices", end="")
        self.submit_batch(queries, show_progress=True)
        print()
        
        # Link to Product nodes
        print(f"   Linking perishable info to products", end="")
        edge_queries = []
        for idx, row in df.iterrows():
            perish_id = f"PERISH_{row['id']}"
            prod_name = self.escape_string(row['product'])
            
            edge_queries.append(f"g.V().has('Product', 'name', '{prod_name}').as('p').V().has('PerishableInfo', 'id', '{perish_id}').coalesce(__.inE('HAS_PERISHABLE_INFO').where(outV().as('p')), addE('HAS_PERISHABLE_INFO').from('p'))")
            
            if len(edge_queries) >= BATCH_SIZE:
                self.submit_batch(edge_queries)
                edge_queries = []
                print(".", end="", flush=True)
        
        if edge_queries:
            self.submit_batch(edge_queries)
        print()
        
        print(f"   ‚úÖ Perishable info loaded and linked")
    
    def load_locations_batch(self):
        """Load locations with batch processing and deduplication"""
        print("\nüìç Loading Locations...")
        
        df = pd.read_sql_table('location', self.pg_engine)
        if SAMPLE_SIZE:
            df = df.head(SAMPLE_SIZE)
        print(f"   Read {len(df)} locations from database")
        
        # Deduplicate
        unique_regions = df['region'].dropna().unique()
        unique_states = df['state'].dropna().unique()
        unique_markets = df['market'].dropna().unique()
        
        print(f"   Unique: {len(unique_regions)} regions, {len(unique_states)} states, {len(unique_markets)} markets")
        
        # Batch create hierarchy vertices
        queries = []
        
        # Regions
        for region in unique_regions:
            region_clean = self.escape_string(region)
            region_id = self.safe_string_for_id(region)
            queries.append(f"g.V().has('Region', 'id', '{region_id}').fold().coalesce(unfold(), addV('Region').property('pk', 'Region').property('id', '{region_id}').property('name', '{region_clean}'))")
        
        # States
        for state in unique_states:
            state_clean = self.escape_string(state)
            state_id = self.safe_string_for_id(state)
            queries.append(f"g.V().has('State', 'id', '{state_id}').fold().coalesce(unfold(), addV('State').property('pk', 'State').property('id', '{state_id}').property('name', '{state_clean}'))")
        
        # Markets
        for market in unique_markets:
            market_clean = self.escape_string(market)
            market_id = self.safe_string_for_id(market)
            queries.append(f"g.V().has('Market', 'id', '{market_id}').fold().coalesce(unfold(), addV('Market').property('pk', 'Market').property('id', '{market_id}').property('name', '{market_clean}'))")
        
        print(f"   Creating {len(queries)} hierarchy vertices", end="")
        self.submit_batch(queries, show_progress=True)
        print()
        
        # Stores
        print(f"   Creating {len(df)} stores", end="")
        store_queries = []
        for idx, row in df.iterrows():
            store_id = self.escape_string(row['location'])
            lat = row['latitude'] if not pd.isna(row['latitude']) else 0
            lon = row['longitude'] if not pd.isna(row['longitude']) else 0
            
            store_queries.append(f"g.V().has('Store', 'id', '{store_id}').fold().coalesce(unfold(), addV('Store').property('pk', 'Store').property('id', '{store_id}').property('name', '{store_id}').property('latitude', {lat}).property('longitude', {lon}))")
            
            if len(store_queries) >= BATCH_SIZE:
                self.submit_batch(store_queries)
                store_queries = []
                print(".", end="", flush=True)
        
        if store_queries:
            self.submit_batch(store_queries)
        print()
        
        # Edges in batches
        print(f"   Creating location relationships", end="")
        edge_queries = []
        for idx, row in df.iterrows():
            region_id = self.safe_string_for_id(row['region'])
            state_id = self.safe_string_for_id(row['state'])
            market_id = self.safe_string_for_id(row['market'])
            store_id = self.escape_string(row['location'])
            
            edge_queries.extend([
                f"g.V().has('Store', 'id', '{store_id}').as('s').V().has('Market', 'id', '{market_id}').coalesce(__.inE('IN_MARKET').where(outV().as('s')), addE('IN_MARKET').from('s'))",
                f"g.V().has('Market', 'id', '{market_id}').as('m').V().has('State', 'id', '{state_id}').coalesce(__.inE('IN_STATE').where(outV().as('m')), addE('IN_STATE').from('m'))",
                f"g.V().has('State', 'id', '{state_id}').as('st').V().has('Region', 'id', '{region_id}').coalesce(__.inE('IN_REGION').where(outV().as('st')), addE('IN_REGION').from('st'))"
            ])
            
            if len(edge_queries) >= BATCH_SIZE:
                self.submit_batch(edge_queries)
                edge_queries = []
                print(".", end="", flush=True)
        
        if edge_queries:
            self.submit_batch(edge_queries)
        print()
        
        print(f"   ‚úÖ Locations loaded")
    
    def get_season(self, month):
        """Determine season from month"""
        month_map = {
            'January': 'Winter', 'February': 'Spring', 'March': 'Spring',
            'April': 'Spring', 'May': 'Summer', 'June': 'Summer',
            'July': 'Summer', 'August': 'Fall', 'September': 'Fall',
            'October': 'Fall', 'November': 'Winter', 'December': 'Winter'
        }
        return month_map.get(str(month), "Unknown")
    
    def load_calendar_batch(self):
        """Load calendar with batch processing (METADATA ONLY for graph)"""
        print("\nüìÖ Loading Calendar (METADATA - year/season/month hierarchy only)...")
        
        df = pd.read_sql_table('calendar', self.pg_engine)
        
        # Only load unique years, months, seasons (not individual dates)
        print(f"   Loading year/month/season hierarchy from {len(df)} dates (full data in PostgreSQL)")
        
        # Create seasons and years (metadata only)
        unique_years = df['year'].dropna().unique()
        unique_months = df['month'].dropna().unique()
        seasons = ['Spring', 'Summer', 'Fall', 'Winter']
        
        queries = []
        
        # Seasons
        for season in seasons:
            queries.append(f"g.V().has('Season', 'name', '{season}').fold().coalesce(unfold(), addV('Season').property('pk', 'Season').property('name', '{season}'))")
        
        # Years
        for year in unique_years:
            queries.append(f"g.V().has('Year', 'year', {int(year)}).fold().coalesce(unfold(), addV('Year').property('pk', 'Year').property('year', {int(year)}))")
        
        # Months
        for month in unique_months:
            month_str = self.escape_string(month)
            season = self.get_season(month)
            queries.extend([
                f"g.V().has('Month', 'name', '{month_str}').fold().coalesce(unfold(), addV('Month').property('pk', 'Month').property('name', '{month_str}'))",
                f"g.V().has('Month', 'name', '{month_str}').as('m').V().has('Season', 'name', '{season}').coalesce(__.inE('IN_SEASON').where(outV().as('m')), addE('IN_SEASON').from('m'))"
            ])
        
        print(f"   Creating calendar hierarchy ({len(seasons)} seasons, {len(unique_years)} years, {len(unique_months)} months)", end="")
        self.submit_batch(queries, show_progress=True)
        print()
        
        print(f"   ‚úÖ Calendar hierarchy loaded (full dates in PostgreSQL)")
    
    def load_events_batch(self):
        """Load unique event types only (metadata for graph)"""
        print("\nüéâ Loading Event Types (metadata only)...")
        
        # Strategy: Load unique event TYPES, not all 17,695 individual event occurrences
        # Full event data (all 17,695 rows) stays in PostgreSQL for queries
        df = pd.read_sql_query("""
            SELECT DISTINCT event, event_type
            FROM events
            ORDER BY event
        """, self.pg_engine)
        
        print(f"   Loading {len(df)} unique event types (full 17,695 event occurrences in PostgreSQL)")
        print(f"   ‚ÑπÔ∏è  Graph stores: Event TYPE metadata (e.g., 'Memorial Day' affects QSR)")
        print(f"   ‚ÑπÔ∏è  PostgreSQL stores: All individual occurrences with dates & stores")
        
        queries = []
        
        for idx, row in df.iterrows():
            event_name = self.escape_string(row['event'])
            event_type = self.escape_string(row['event_type'])
            event_id = self.safe_string_for_id(row['event'])
            
            queries.append(f"g.V().has('EventType', 'id', '{event_id}').fold().coalesce(unfold(), addV('EventType').property('pk', 'EventType').property('id', '{event_id}').property('name', '{event_name}').property('type', '{event_type}'))")
            
            if len(queries) >= BATCH_SIZE:
                self.submit_batch(queries)
                queries = []
                print(".", end="", flush=True)
        
        if queries:
            self.submit_batch(queries)
        print()
        
        print(f"   ‚úÖ Event types loaded ({len(df)} unique events, not 17K individual occurrences!)")
    
    def load_weather_metadata(self):
        """Load weather condition vertices (metadata only)"""
        print("\n‚òÅÔ∏è Loading Weather Conditions (metadata)...")
        
        # Just create weather condition types
        conditions = ['Heatwave', 'ColdSpell', 'HeavyRain', 'Snow', 'Normal']
        queries = []
        
        for condition in conditions:
            queries.append(f"g.V().has('WeatherCondition', 'name', '{condition}').fold().coalesce(unfold(), addV('WeatherCondition').property('pk', 'WeatherCondition').property('name', '{condition}'))")
        
        self.submit_batch(queries)
        print(f"   ‚úÖ {len(conditions)} weather conditions loaded")
    
    def load_metrics_metadata(self):
        """Load weather-sensitive product-store relationships (metadata only)"""
        print("\nüìà Loading Metrics Metadata (high-variance products)...")
        print("   ‚ÑπÔ∏è  Full data in PostgreSQL, only high-variance relationships in graph")
        
        # Sample high-variance metrics
        df = pd.read_sql_query("""
            SELECT product, location, 
                   AVG(metric) as avg_metric,
                   STDDEV(metric) as stddev_metric
            FROM metrics
            GROUP BY product, location
            HAVING STDDEV(metric) > 20
            LIMIT 500
        """, self.pg_engine)
        
        print(f"   Found {len(df)} high-variance product-store pairs")
        
        queries = []
        for idx, row in df.iterrows():
            # Find product by name instead of ID
            product_name = self.escape_string(row['product'])
            store_id = self.escape_string(row['location'])
            avg_metric = float(row['avg_metric']) if not pd.isna(row['avg_metric']) else 0
            
            queries.append(f"g.V().has('Product', 'name', '{product_name}').as('p').V().has('Store', 'id', '{store_id}').coalesce(__.inE('WEATHER_SENSITIVE').where(outV().as('p')), addE('WEATHER_SENSITIVE').property('avg_metric', {avg_metric}).from('p'))")
            
            if len(queries) >= BATCH_SIZE:
                self.submit_batch(queries)
                queries = []
        
        if queries:
            self.submit_batch(queries)
        
        print(f"   ‚úÖ Metrics metadata loaded")
    
    def load_inventory_metadata(self):
        """Load inventory system metadata (sales, batches, tracking, spoilage)"""
        print("\nüì¶ Loading Inventory Metadata...")
        print("   ‚ÑπÔ∏è  Metadata-only approach for high-volume fact tables")
        
        # Get batch statistics by product
        batch_df = pd.read_sql_query("""
            SELECT 
                product_code,
                COUNT(DISTINCT batch_id) as batch_count,
                COUNT(CASE WHEN expiry_date IS NOT NULL 
                      AND expiry_date <= CURRENT_DATE + INTERVAL '30 days' 
                      AND stock_at_week_end > 0 THEN 1 END) as expiring_soon_count
            FROM batches
            GROUP BY product_code
            HAVING COUNT(DISTINCT batch_id) > 5
        """, self.pg_engine)
        
        print(f"   Found {len(batch_df)} products with batch tracking")
        
        # Create BatchInfo vertices and edges
        queries = []
        for idx, row in batch_df.iterrows():
            product_code = int(row['product_code'])
            batch_count = int(row['batch_count'])
            expiring_soon = int(row['expiring_soon_count']) if not pd.isna(row['expiring_soon_count']) else 0
            
            # Add BatchInfo vertex
            queries.append(
                f"g.addV('BatchInfo').property('product_code', {product_code})"
                f".property('batch_count', {batch_count})"
                f".property('expiring_soon', {expiring_soon})"
            )
            
            # Link to Product
            queries.append(
                f"g.V().hasLabel('Product').has('product_id', {product_code}).as('p')"
                f".V().hasLabel('BatchInfo').has('product_code', {product_code}).as('bi')"
                f".addE('HAS_BATCH_TRACKING').from('p').to('bi')"
            )
            
            if len(queries) >= BATCH_SIZE:
                self.submit_batch(queries)
                queries = []
                print(".", end="", flush=True)
        
        if queries:
            self.submit_batch(queries)
        print()
        
        # Get spoilage patterns
        spoilage_df = pd.read_sql_query("""
            SELECT 
                product_code,
                COUNT(*) as batch_count,
                SUM(spoilage_qty) as total_spoiled,
                AVG(spoilage_pct) as avg_spoilage_pct,
                SUM(CASE WHEN spoilage_case = 1 THEN 1 ELSE 0 END) as spoilage_cases
            FROM spoilage_report
            WHERE spoilage_qty > 0
            GROUP BY product_code
        """, self.pg_engine)
        
        print(f"   Found {len(spoilage_df)} products with spoilage data")
        
        queries = []
        for idx, row in spoilage_df.iterrows():
            product_code = int(row['product_code'])
            batch_count = int(row['batch_count'])
            total_spoiled = int(row['total_spoiled']) if not pd.isna(row['total_spoiled']) else 0
            avg_spoilage_pct = float(row['avg_spoilage_pct']) if not pd.isna(row['avg_spoilage_pct']) else 0.0
            spoilage_cases = int(row['spoilage_cases']) if not pd.isna(row['spoilage_cases']) else 0
            
            # Add SpoilagePattern vertex
            queries.append(
                f"g.addV('SpoilagePattern').property('product_code', {product_code})"
                f".property('batch_count', {batch_count})"
                f".property('total_spoiled', {total_spoiled})"
                f".property('avg_spoilage_pct', {avg_spoilage_pct})"
                f".property('spoilage_cases', {spoilage_cases})"
            )
            
            # Link to Product
            queries.append(
                f"g.V().hasLabel('Product').has('product_id', {product_code}).as('p')"
                f".V().hasLabel('SpoilagePattern').has('product_code', {product_code}).as('sp')"
                f".addE('HAS_SPOILAGE_PATTERN').from('p').to('sp')"
            )
            
            if len(queries) >= BATCH_SIZE:
                self.submit_batch(queries)
                queries = []
                print(".", end="", flush=True)
        
        if queries:
            self.submit_batch(queries)
        print()
        
        # Get sales patterns
        sales_df = pd.read_sql_query("""
            SELECT 
                product_code,
                COUNT(*) as transaction_count,
                SUM(sales_units) as total_units_sold,
                SUM(total_amount) as total_revenue,
                AVG(total_amount) as avg_transaction_value
            FROM sales
            GROUP BY product_code
            HAVING COUNT(*) > 100
            LIMIT 100
        """, self.pg_engine)
        
        print(f"   Found {len(sales_df)} products with significant sales volume")
        
        queries = []
        for idx, row in sales_df.iterrows():
            product_code = int(row['product_code'])
            transaction_count = int(row['transaction_count'])
            total_units_sold = int(row['total_units_sold']) if not pd.isna(row['total_units_sold']) else 0
            total_revenue = float(row['total_revenue']) if not pd.isna(row['total_revenue']) else 0.0
            avg_transaction_value = float(row['avg_transaction_value']) if not pd.isna(row['avg_transaction_value']) else 0.0
            
            # Add SalesPattern vertex
            queries.append(
                f"g.addV('SalesPattern').property('product_code', {product_code})"
                f".property('transaction_count', {transaction_count})"
                f".property('total_units_sold', {total_units_sold})"
                f".property('total_revenue', {total_revenue})"
                f".property('avg_transaction_value', {avg_transaction_value})"
            )
            
            # Link to Product
            queries.append(
                f"g.V().hasLabel('Product').has('product_id', {product_code}).as('p')"
                f".V().hasLabel('SalesPattern').has('product_code', {product_code}).as('sp')"
                f".addE('HAS_SALES_PATTERN').from('p').to('sp')"
            )
            
            if len(queries) >= BATCH_SIZE:
                self.submit_batch(queries)
                queries = []
                print(".", end="", flush=True)
        
        if queries:
            self.submit_batch(queries)
        print()
        
        print(f"   ‚úÖ Inventory metadata loaded")
    
    def verify_graph(self):
        """Verify graph creation"""
        print("\nüîç Verifying graph...")
        
        try:
            labels = ['Region', 'State', 'Market', 'Store', 'Department', 'Category', 
                     'Product', 'PerishableInfo', 'Year', 'Month', 'Season', 'EventType', 'WeatherCondition',
                     'BatchInfo', 'SpoilagePattern', 'SalesPattern']
            
            vertex_counts = {}
            for label in labels:
                result = self.submit_query(f"g.V().hasLabel('{label}').count()")
                vertex_counts[label] = result[0] if result else 0
            
            total_vertices = self.submit_query("g.V().count()")[0] if self.submit_query("g.V().count()") else 0
            total_edges = self.submit_query("g.E().count()")[0] if self.submit_query("g.E().count()") else 0
            
            print(f"\nüìä Graph Statistics:")
            print(f"   Total Vertices: {total_vertices:,}")
            print(f"   Total Edges: {total_edges:,}")
            print(f"\n   Vertex Breakdown:")
            for label, count in vertex_counts.items():
                if count > 0:
                    print(f"     {label}: {count:,}")
        except Exception as e:
            print(f"‚ùå Verification error: {e}")


def main():
    print("="*80)
    print("üöÄ PLANALYTICS GREMLIN GRAPH BUILDER - ASYNC OPTIMIZED")
    print("="*80)
    print(f"\nConfiguration:")
    print(f"  Batch Size: {BATCH_SIZE}")
    print(f"  Sample Size: {SAMPLE_SIZE if SAMPLE_SIZE else 'All data'}")
    print(f"  Max Retries: {MAX_RETRIES}")
    
    builder = AsyncPlanalyticsGremlinBuilder()
    
    try:
        # Clear graph
        builder.drop_graph()
        
        print(f"\nüìã Loading data from PostgreSQL...")
        
        # Load all data
        builder.load_product_hierarchy_batch()
        builder.load_perishable_batch()
        builder.load_locations_batch()
        builder.load_calendar_batch()  # Sample
        builder.load_events_batch()  # Sample
        builder.load_weather_metadata()  # Metadata only
        builder.load_metrics_metadata()  # Metadata only
        builder.load_inventory_metadata()  # NEW: Inventory metadata only
        
        # Verify
        builder.verify_graph()
        
        print("\n" + "="*80)
        print("‚úÖ PLANALYTICS KNOWLEDGE GRAPH BUILT SUCCESSFULLY!")
        print("="*80)
        print("\nüìã OPTIMIZED METADATA-BASED APPROACH:")
        print("  ‚úÖ Full product hierarchy (dept ‚Üí category ‚Üí product): 37 products")
        print("  ‚úÖ Full perishable data: 8 products with shelf life")
        print("  ‚úÖ Full location hierarchy (region ‚Üí state ‚Üí market ‚Üí store): 183 stores")
        print("  ‚úÖ Calendar hierarchy (year ‚Üí month ‚Üí season): Metadata only")
        print("  ‚úÖ Event types: Unique event names only (not 17K individual events)")
        print("  ‚úÖ Weather conditions: 5 condition types")
        print("  ‚úÖ Metrics: High-variance relationships only")
        print("  ‚úÖ Inventory: Batch tracking, spoilage patterns, sales patterns (metadata only)")
        print("\nüí° WHY THIS APPROACH?")
        print("  ‚Ä¢ Small dimension tables (products, locations): FULL DATA ‚Üí Prevents LLM hallucination")
        print("  ‚Ä¢ Large fact tables (events, weather, metrics, sales, batches): METADATA ONLY ‚Üí Too large for graph")
        print("  ‚Ä¢ Full transactional data: In PostgreSQL for SQL queries")
        print("  ‚Ä¢ Graph purpose: Entity resolution + relationship discovery")
        print("\nüí° GRAPH RELATIONSHIPS:")
        print("  ‚Ä¢ (Product) -[:HAS_BATCH_TRACKING]-> (BatchInfo)")
        print("  ‚Ä¢ (Product) -[:HAS_SPOILAGE_PATTERN]-> (SpoilagePattern)")
        print("  ‚Ä¢ (Product) -[:HAS_SALES_PATTERN]-> (SalesPattern)")
        print("\n‚ö° Expected build time: 2-3 minutes (was 30+ minutes with full events!)")
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        builder.close()


if __name__ == "__main__":
    main()
