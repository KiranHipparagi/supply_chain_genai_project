"""Context Resolver Service - Combines Azure Search entity resolution with Gremlin graph expansion"""
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from database.azure_search import azure_search
from database.gremlin_db import gremlin_conn
from core.logger import logger

# Static current date context for demo data (November 8, 2025)
CURRENT_WEEKEND_DATE = datetime(2025, 11, 8)
CURRENT_WEEK_END = "2025-11-08"


class ContextResolver:
    """
    Hybrid context resolution using Azure AI Search + Gremlin Knowledge Graph
    
    Workflow:
    1. Azure Search: Resolve vague terms to exact IDs (e.g., "Pepsi" ‚Üí product_id)
    2. Gremlin Graph: Expand context via relationships (e.g., find all products in same category)
    3. Return enriched context for SQL generation
    
    Date Context:
    - Current Weekend: November 8, 2025 (static for demo data)
    - Next Week: November 15, 2025
    - Last Week: November 1, 2025
    - Next Month: December 2025
    - Last Month: October 2025
    """
    
    def __init__(self):
        self.search = azure_search
        self.graph = gremlin_conn
        # Static date context
        self.current_date = CURRENT_WEEKEND_DATE
        self.current_week_end = CURRENT_WEEK_END
    
    def resolve_query_context(self, user_query: str) -> Dict[str, Any]:
        """
        Main entry point: Resolve full context from natural language query
        
        Args:
            user_query: Natural language query from user
            
        Returns:
            {
                "products": {
                    "resolved": [...],  # From Azure Search
                    "expanded": [...]   # From Gremlin graph
                },
                "locations": {
                    "resolved": [...],
                    "expanded": [...]
                },
                "dates": {
                    "resolved": [...],
                    "date_range": (start, end)
                },
                "events": {
                    "resolved": [...],
                    "related_events": [...]
                },
                "metadata": {
                    "sales_coverage": [...],
                    "weather_coverage": [...],
                    "available_metrics": [...]
                }
            }
        """
        print("\n" + "="*80)
        print("üîé STEP 1: CONTEXT RESOLUTION - Starting RAG Pipeline")
        print("="*80)
        print(f"üìù User Query: {user_query}")
        logger.info(f"üîç Resolving context for query: {user_query}")
        
        # Step 1: Entity resolution via Azure Search
        print("\nüîµ STEP 1.1: Azure AI Search - Entity Resolution")
        print("-" * 80)
        entities = self.search.resolve_entities(user_query)
        
        # Detailed printing of resolved entities
        print(f"‚úÖ Entities found:")
        if entities.get('products'):
            print(f"   ‚Ä¢ Products ({len(entities['products'])}): {[p.get('product', p.get('product_name', 'Unknown')) for p in entities['products'][:5]]}" + (f" ... (+{len(entities['products'])-5} more)" if len(entities['products']) > 5 else ""))
        else:
            print("   ‚Ä¢ Products: None")
            
        if entities.get('locations'):
            print(f"   ‚Ä¢ Locations ({len(entities['locations'])}): {[l.get('location', l.get('store_name', 'Unknown')) for l in entities['locations'][:5]]}" + (f" ... (+{len(entities['locations'])-5} more)" if len(entities['locations']) > 5 else ""))
        else:
            print("   ‚Ä¢ Locations: None")
            
        if entities.get('events'):
            print(f"   ‚Ä¢ Events ({len(entities['events'])}): {[e.get('event', e.get('event_name', 'Unknown')) for e in entities['events'][:5]]}" + (f" ... (+{len(entities['events'])-5} more)" if len(entities['events']) > 5 else ""))
        else:
            print("   ‚Ä¢ Events: None")
            
        if entities.get('dates'):
            print(f"   ‚Ä¢ Dates ({len(entities['dates'])}): {[d.get('date') for d in entities['dates'][:5]]}" + (f" ... (+{len(entities['dates'])-5} more)" if len(entities['dates']) > 5 else ""))
        else:
            print("   ‚Ä¢ Dates: None")
        
        # Step 2: Context expansion via Gremlin
        print("\nüü¢ STEP 1.2: Gremlin Knowledge Graph - Context Expansion")
        print("-" * 80)
        expanded_context = self._expand_context_via_graph(entities)
        
        # Detailed printing of expanded context
        print(f"‚úÖ Graph expansion complete:")
        if expanded_context.get('expanded_products'):
            print(f"   ‚Ä¢ Expanded Products ({len(expanded_context['expanded_products'])}): {[p.get('product_name') for p in expanded_context['expanded_products'][:5]]}" + (f" ... (+{len(expanded_context['expanded_products'])-5} more)" if len(expanded_context['expanded_products']) > 5 else ""))
        else:
            print("   ‚Ä¢ Expanded Products: None")
            
        if expanded_context.get('expanded_locations'):
            print(f"   ‚Ä¢ Expanded Locations ({len(expanded_context['expanded_locations'])}): {[l.get('store_name') for l in expanded_context['expanded_locations'][:5]]}" + (f" ... (+{len(expanded_context['expanded_locations'])-5} more)" if len(expanded_context['expanded_locations']) > 5 else ""))
        else:
            print("   ‚Ä¢ Expanded Locations: None")
        
        # Step 3: Get metadata context
        metadata = self.search.get_schema_context(user_query)
        
        # Step 4: Combine everything
        full_context = {
            "products": {
                "resolved": entities.get("products", []),
                "expanded": expanded_context.get("expanded_products", [])
            },
            "locations": {
                "resolved": entities.get("locations", []),
                "expanded": expanded_context.get("expanded_locations", [])
            },
            "dates": {
                "resolved": entities.get("dates", []),
                "date_range": self._extract_date_range(entities.get("dates", []))
            },
            "events": {
                "resolved": entities.get("events", []),
                "related_events": expanded_context.get("related_events", [])
            },
            "metadata": metadata
        }
        
        logger.info(f"‚úÖ Context resolved successfully")
        return full_context
    
    def _expand_context_via_graph(self, entities: Dict[str, List]) -> Dict[str, Any]:
        """Expand entity context using Gremlin knowledge graph relationships"""
        print("  üåê Querying Gremlin Graph Database...")
        if not self.graph.ensure_connected():
            logger.warning("‚ö†Ô∏è Gremlin unavailable - skipping graph expansion (system will use Azure Search only)")
            print("  ‚ö†Ô∏è  Gremlin Graph unavailable - using Azure Search context only")
            return {
                "expanded_products": [],
                "expanded_locations": [],
                "related_events": []
            }
        
        expanded = {}
        
        # Expand products via category hierarchy
        product_ids = [p.get("id") for p in entities.get("products", []) if p.get("id")]
        if product_ids:
            print(f"  üì¶ Expanding product context for IDs: {product_ids[:3]}...")
            expanded["expanded_products"] = self.graph.expand_product_context(product_ids)
            print(f"  ‚úÖ Found {len(expanded['expanded_products'])} related products via graph traversal")
        else:
            expanded["expanded_products"] = []
        
        # Expand locations via geographic hierarchy
        location_ids = [l.get("id") for l in entities.get("locations", []) if l.get("id")]
        if location_ids:
            print(f"  üè¢ Expanding location context for IDs: {location_ids[:3]}...")
            expanded["expanded_locations"] = self.graph.expand_location_context(location_ids)
            print(f"  ‚úÖ Found {len(expanded['expanded_locations'])} related locations via graph traversal")
        else:
            expanded["expanded_locations"] = []
        
        # Find related events
        date_list = [d.get("date") for d in entities.get("dates", []) if d.get("date")]
        if date_list and location_ids:
            expanded["related_events"] = self.graph.find_related_events(location_ids, date_list)
        else:
            expanded["related_events"] = []
        
        return expanded
    
    def _convert_excel_date(self, val: Any) -> Any:
        """Convert Excel serial date to YYYY-MM-DD string"""
        try:
            if isinstance(val, (int, float)):
                return (datetime(1899, 12, 30) + timedelta(days=val)).strftime('%Y-%m-%d')
            elif isinstance(val, str) and val.isdigit():
                return (datetime(1899, 12, 30) + timedelta(days=int(val))).strftime('%Y-%m-%d')
            return val
        except Exception:
            return val

    def _extract_date_range(self, dates: List[Dict]) -> Optional[tuple]:
        """Extract min/max date range from calendar results"""
        if not dates:
            return None
        
        date_values = [self._convert_excel_date(d.get("date")) for d in dates if d.get("date")]
        if not date_values:
            return None
        
        return (min(date_values), max(date_values))
    
    def get_sql_generation_prompt(self, user_query: str, context: Dict[str, Any]) -> str:
        """
        Generate a comprehensive prompt for SQL generation with full context
        
        This replaces the hardcoded schema prompt in database_agent.py
        """
        prompt_parts = []
        
        # Add static date context at the top (CRITICAL for relative date queries)
        prompt_parts.append("=== CURRENT DATE CONTEXT ===")
        prompt_parts.append(f"Current Weekend (Week End Date): {self.current_week_end} (November 8, 2025)")
        prompt_parts.append("- 'This week' or 'current week' = end_date '2025-11-08'")
        prompt_parts.append("- 'Next week' or 'NW' = end_date '2025-11-15'")
        prompt_parts.append("- 'Last week' or 'LW' = end_date '2025-11-01'")
        prompt_parts.append("- 'Next month' or 'NM' = December 2025 ‚Üí Use: c.month = 'December' AND c.year = 2025")
        prompt_parts.append("- 'Last month' or 'LM' = October 2025 ‚Üí Use: c.month = 'October' AND c.year = 2025")
        prompt_parts.append("- 'Last year' or 'LY' = 2024")
        prompt_parts.append("- 'Next 2 weeks' = end_dates '2025-11-15' and '2025-11-22'")
        prompt_parts.append("- 'Last 6 weeks' = 6 weeks ending before '2025-11-08'")
        prompt_parts.append("- CRITICAL: calendar.month column is STRING ('January', 'December'), NOT integer!\n")
        
        # User query
        prompt_parts.append(f"User Query: {user_query}\n")
        
        # Products context
        products = context.get("products", {})
        if products.get("resolved"):
            # CRITICAL: Detect if user is asking about CATEGORY/DEPARTMENT or SPECIFIC PRODUCTS
            # Azure Search returns product matches, but we need to determine user's intent
            product_info = []
            product_names = []
            categories = set()
            departments = set()
            
            for p in products["resolved"][:10]:  # Check all returned products
                name = p.get("product", p.get("product_name", p.get("id", "Unknown")))
                cat = p.get("category", "Unknown")
                dept = p.get("dept", "Unknown")
                product_info.append(f"{name} (Dept: {dept}, Category: {cat})")
                product_names.append(name)
                categories.add(cat)
                departments.add(dept)
            
            product_ids = [p.get("product_id", p.get("id")) for p in products["resolved"][:10]]
            
            prompt_parts.append(f"Relevant Products: {', '.join(product_info)}")
            prompt_parts.append(f"Product Names Found: {product_names}")
            prompt_parts.append(f"Categories Found: {list(categories)}")
            prompt_parts.append(f"Departments Found: {list(departments)}")
            prompt_parts.append(f"Product IDs: {product_ids}")
            
            prompt_parts.append("\nüö®üö® CRITICAL PRODUCT FILTERING RULES - READ CAREFULLY:")
            prompt_parts.append("")
            prompt_parts.append("=== STEP 1: IDENTIFY USER'S INTENT ===")
            prompt_parts.append("Analyze the User Query above and determine what level the user is asking about:")
            prompt_parts.append("")
            prompt_parts.append("üì¶ OPTION A: User mentions a CATEGORY or DEPARTMENT")
            prompt_parts.append("   Keywords in query: 'QSR', 'Perishable', 'Fast Food', 'Grocery', 'Beverages', 'Dairy', etc.")
            prompt_parts.append("   Examples:")
            prompt_parts.append("   - 'Which QSR products...' ‚Üí User is asking about CATEGORY = 'QSR'")
            prompt_parts.append("   - 'How many products in QSR category?' ‚Üí CATEGORY = 'QSR'")
            prompt_parts.append("   - 'Perishable items' ‚Üí CATEGORY = 'Perishable'")
            prompt_parts.append("   - 'Fast Food department' ‚Üí DEPT = 'Fast Food'")
            prompt_parts.append("   - 'Grocery products' ‚Üí DEPT = 'Grocery'")
            prompt_parts.append("")
            prompt_parts.append("üì¶ OPTION B: User mentions SPECIFIC PRODUCT NAMES")
            prompt_parts.append("   Keywords in query: 'Hamburgers', 'Milk', 'Sandwiches', 'Eggs', 'Coffee', etc.")
            prompt_parts.append("   Examples:")
            prompt_parts.append("   - 'Sandwiches and Salads' ‚Üí User is asking about specific products")
            prompt_parts.append("   - 'How did Hamburgers perform?' ‚Üí Specific product = 'Hamburgers'")
            prompt_parts.append("   - 'Milk sales' ‚Üí Specific product = 'Milk'")
            prompt_parts.append("")
            prompt_parts.append("=== STEP 2: APPLY CORRECT SQL FILTER ===")
            prompt_parts.append("")
            prompt_parts.append("‚úÖ IF USER ASKS ABOUT CATEGORY (Option A):")
            prompt_parts.append("   ‚Üí Filter on: WHERE ph.category = 'category_name'")
            prompt_parts.append("   ‚Üí DO NOT filter on specific product names!")
            prompt_parts.append("   ‚Üí Let the database return ALL products in that category")
            prompt_parts.append("   Examples:")
            prompt_parts.append("   - 'QSR products' ‚Üí WHERE ph.category = 'QSR'")
            prompt_parts.append("   - 'Perishable items' ‚Üí WHERE ph.category = 'Perishable'")
            prompt_parts.append("   - 'How many QSR products?' ‚Üí SELECT COUNT(*) FROM product_hierarchy WHERE category = 'QSR'")
            prompt_parts.append("")
            prompt_parts.append("‚úÖ IF USER ASKS ABOUT DEPARTMENT (Option A):")
            prompt_parts.append("   ‚Üí Filter on: WHERE ph.dept = 'dept_name'")
            prompt_parts.append("   ‚Üí DO NOT filter on specific product names!")
            prompt_parts.append("   Examples:")
            prompt_parts.append("   - 'Fast Food products' ‚Üí WHERE ph.dept = 'Fast Food'")
            prompt_parts.append("   - 'Grocery department' ‚Üí WHERE ph.dept = 'Grocery'")
            prompt_parts.append("")
            prompt_parts.append("‚úÖ IF USER ASKS ABOUT SPECIFIC PRODUCTS (Option B):")
            prompt_parts.append("   ‚Üí Filter on: WHERE ph.product IN ('product1', 'product2', ...)")
            prompt_parts.append("   ‚Üí ONLY include products user explicitly mentioned in the query")
            prompt_parts.append("   ‚Üí DO NOT include all products from Azure Search results!")
            prompt_parts.append("   Examples:")
            prompt_parts.append("   - 'Sandwiches' ‚Üí WHERE ph.product = 'Sandwiches'")
            prompt_parts.append("   - 'Sandwiches and Salads' ‚Üí WHERE ph.product IN ('Sandwiches', 'Salads')")
            prompt_parts.append("   - 'Milk, Eggs, and Bacon' ‚Üí WHERE ph.product IN ('Milk', 'Eggs', 'Bacon')")
            prompt_parts.append("")
            prompt_parts.append("=== STEP 3: EXAMPLES OF CORRECT vs WRONG SQL ===")
            prompt_parts.append("")
            prompt_parts.append("‚ùå WRONG - User asks 'QSR products' but you filter on specific products:")
            prompt_parts.append("   SELECT ... WHERE ph.product IN ('Sandwiches', 'Salads', 'Chicken')  ‚Üê WRONG!")
            prompt_parts.append("   Problem: Misses Hamburgers, Pizza, Coffee & Tea, Desserts, Smoothies!")
            prompt_parts.append("")
            prompt_parts.append("‚úÖ CORRECT - User asks 'QSR products':")
            prompt_parts.append("   SELECT ... WHERE ph.category = 'QSR'  ‚Üê CORRECT!")
            prompt_parts.append("   Returns: ALL 8 QSR products (Hamburgers, Chicken, Coffee & Tea, Desserts, Sandwiches, Smoothies, Salads, Pizza)")
            prompt_parts.append("")
            prompt_parts.append("‚ùå WRONG - User asks 'How many products in QSR category?':")
            prompt_parts.append("   SELECT COUNT(*) ... WHERE ph.product IN ('Sandwiches', 'Salads')  ‚Üê WRONG!")
            prompt_parts.append("   Result: Returns 2 instead of 8")
            prompt_parts.append("")
            prompt_parts.append("‚úÖ CORRECT - User asks 'How many products in QSR category?':")
            prompt_parts.append("   SELECT COUNT(DISTINCT ph.product) FROM product_hierarchy WHERE ph.category = 'QSR'  ‚Üê CORRECT!")
            prompt_parts.append("   Result: Returns 8 (all QSR products)")
            prompt_parts.append("")
            prompt_parts.append("‚ùå WRONG - User asks 'Perishable items':")
            prompt_parts.append("   SELECT ... WHERE ph.product IN ('Milk', 'Eggs')  ‚Üê WRONG! Misses Bacon")
            prompt_parts.append("")
            prompt_parts.append("‚úÖ CORRECT - User asks 'Perishable items':")
            prompt_parts.append("   SELECT ... WHERE ph.category = 'Perishable'  ‚Üê CORRECT!")
            prompt_parts.append("   OR join with perishable table: JOIN perishable p ON ph.product = p.product")
            prompt_parts.append("")
            prompt_parts.append("‚ùå WRONG - User asks 'Sandwiches performance' but you filter on category:")
            prompt_parts.append("   SELECT ... WHERE ph.category = 'QSR'  ‚Üê WRONG! Returns all QSR, not just Sandwiches")
            prompt_parts.append("")
            prompt_parts.append("‚úÖ CORRECT - User asks 'Sandwiches performance':")
            prompt_parts.append("   SELECT ... WHERE ph.product = 'Sandwiches'  ‚Üê CORRECT!")
            prompt_parts.append("")
            prompt_parts.append("=== SPECIAL CASES ===")
            prompt_parts.append("")
            prompt_parts.append("üîç CASE 1: User says 'QSR products like Sandwiches and Salads'")
            prompt_parts.append("   ‚Üí This is asking about SPECIFIC PRODUCTS, not whole category")
            prompt_parts.append("   ‚Üí Use: WHERE ph.product IN ('Sandwiches', 'Salads')")
            prompt_parts.append("")
            prompt_parts.append("üîç CASE 2: User says 'All QSR products'")
            prompt_parts.append("   ‚Üí This is asking about ENTIRE CATEGORY")
            prompt_parts.append("   ‚Üí Use: WHERE ph.category = 'QSR'")
            prompt_parts.append("")
            prompt_parts.append("üîç CASE 3: User says 'What department are perishable items in?'")
            prompt_parts.append("   ‚Üí This is asking about CATEGORY = 'Perishable'")
            prompt_parts.append("   ‚Üí Use: SELECT ph.dept FROM product_hierarchy WHERE ph.category = 'Perishable' GROUP BY ph.dept")
            prompt_parts.append("   ‚Üí OR: JOIN perishable table to show all perishable products")
            prompt_parts.append("")
            prompt_parts.append("üîç CASE 4: Counting products in a category")
            prompt_parts.append("   ‚Üí User asks: 'How many products in QSR?', 'Count of Perishable items', etc.")
            prompt_parts.append("   ‚Üí Use: SELECT COUNT(DISTINCT ph.product) FROM product_hierarchy WHERE ph.category = 'category_name'")
            prompt_parts.append("   ‚Üí DO NOT filter on specific product names!")
            prompt_parts.append("")
            prompt_parts.append("=== DECISION TREE ===")
            prompt_parts.append("")
            prompt_parts.append("1. Check User Query for category/department keywords ('QSR', 'Perishable', 'Fast Food', 'Grocery')")
            prompt_parts.append("   ‚Üí YES ‚Üí Use: WHERE ph.category = '...' OR WHERE ph.dept = '...'")
            prompt_parts.append("   ‚Üí NO ‚Üí Continue to step 2")
            prompt_parts.append("")
            prompt_parts.append("2. Check if user mentioned specific product names ('Hamburgers', 'Milk', 'Sandwiches')")
            prompt_parts.append("   ‚Üí YES ‚Üí Use: WHERE ph.product IN ('product1', 'product2', ...)")
            prompt_parts.append("   ‚Üí Count how many products user mentioned, use ONLY those")
            prompt_parts.append("")
            prompt_parts.append("3. If unclear, prefer CATEGORY/DEPARTMENT filter over specific products")
            prompt_parts.append("   ‚Üí It's better to return all products in a category than to miss some")
            prompt_parts.append("")
            prompt_parts.append(f"Available categories in this query: {list(categories)}")
            prompt_parts.append(f"Available departments in this query: {list(departments)}")
            prompt_parts.append(f"Available products in this query: {product_names}\n")
        
        if products.get("expanded"):
            # DO NOT include expanded products - user wants ONLY what they mentioned!
            # Expansion can add unwanted products like 'Salads' when user only asked for 'Sandwiches'
            pass  # Intentionally disabled to prevent over-filtering
        
        # Locations context
        locations = context.get("locations", {})
        if locations.get("resolved"):
            # Use 'location' field (fallback to 'store_name' or 'id')
            location_names = [l.get("location", l.get("store_name", l.get("id", "Unknown"))) for l in locations["resolved"][:5]]
            location_ids = [l.get("id") for l in locations["resolved"][:20]]
            prompt_parts.append(f"Relevant Locations: {', '.join(str(x) for x in location_names)}")
            prompt_parts.append(f"Store IDs to filter: {location_ids}\n")
        
        if locations.get("expanded"):
            expanded_ids = [l.get("store_id") for l in locations["expanded"][:50]]
            prompt_parts.append(f"Related Locations (same region/market): {len(expanded_ids)} stores")
            prompt_parts.append(f"Expanded Store IDs: {expanded_ids}\n")
        
        # Date context
        dates = context.get("dates", {})
        if dates.get("date_range"):
            start_date, end_date = dates["date_range"]
            prompt_parts.append(f"Date Range: {start_date} to {end_date}\n")
        elif dates.get("resolved"):
            date_samples = [d.get("date") for d in dates["resolved"][:5]]
            prompt_parts.append(f"Relevant Dates: {date_samples}\n")
        
        # Events context
        events = context.get("events", {})
        if events.get("resolved"):
            # Use 'event' field (fallback to 'event_name')
            event_names = [e.get("event", e.get("event_name", "Unknown Event")) for e in events["resolved"][:5]]
            prompt_parts.append(f"Relevant Events: {', '.join(str(x) for x in event_names)}\n")
        
        # Database schema (dynamic based on query needs)
        prompt_parts.append("\nAvailable Database Tables:")
        prompt_parts.append("""
=== QUERY TYPE DETECTION (CRITICAL!) ===
FIRST, carefully determine which type of data the user needs:

**TYPE 1: SALES PERFORMANCE ANALYSIS** ‚Üí Use 'sales' table
   Primary Keywords: "sales", "sold", "revenue", "perform", "performance", "growing", "fastest growing", 
                     "units sold", "sales amount", "traffic", "transactions", "change in sales"
   Questions like: "How did X perform?", "sales of X changed", "fastest growing demand", "units sold"
   Example: "How did Smoothies perform in Charlotte during heatwave weeks?"
   SQL Strategy: Use sales table, join with product_hierarchy, location, calendar

**TYPE 2: WDD + SALES COMBINED ANALYSIS** ‚Üí Join 'metrics' AND 'sales' tables
   Keywords: "weather affected sales", "weather affect demand", "weather impact/impacts on", "due to weather", 
             "weather impact on sales", "weather-driven sales", "compared to weather", "because of weather", 
             "weather and sales", "based on weather", "based on the weather", "weather + performance/YoY"
   Questions like: "How much was due to weather?", "weather impacts on traffic", "weather affect demand",
                   "based on weather, what products had best performance"
   Example: "Sandwich sales were down. How much was due to weather?"
   Example: "How much did weather affect demand in Apparel this summer?"
   Example: "Based on the weather, what grocery products had the best year-over-year performance?"
   SQL Strategy: Join metrics and sales tables to compare WDD trend vs actual sales performance
   
**TYPE 3: WEATHER-DRIVEN DEMAND ONLY** ‚Üí Use 'metrics' table only
   Keywords: "WDD", "demand forecast", "weather-driven demand", "seasonal demand", "weather trend"
   Questions like: "What's the weather-driven demand trend?"
   Example: "Show WDD for ice cream this summer"
   SQL Strategy: Use metrics table for trend analysis (no sales join needed)

**TYPE 4: INVENTORY/BATCH TRACKING** ‚Üí Use 'batches' + 'batch_stock_tracking' tables
   Keywords: "inventory", "stock", "batch", "expiry", "expiring", "stock level", "stock movements"
   Example: "What batches are expiring next week?"
   
**TYPE 5: SPOILAGE/WASTE** ‚Üí Use 'spoilage_report' table
   Keywords: "spoilage", "waste", "spoiled", "expired products", "loss"
   Example: "Which stores have highest spoilage?"

**TYPE 6: EVENT-BASED QUERIES** ‚Üí Use 'events' table + 'sales' table
   Keywords: "festival", "concert", "game", "holiday", "event", specific event names
   CRITICAL: Search BOTH event AND event_type columns!
   Example WHERE: (e.event ILIKE '%music%' OR e.event_type ILIKE '%music%' OR e.event_type ILIKE '%festival%')
   Example: "Music festival in Nashville" ‚Üí Search event column AND event_type column

**TYPE 6: EVENT-BASED ANALYSIS** ‚Üí Use 'events' table joined with sales or metrics
   Keywords: "event", "festival", "holiday", "concert", "game", "football", "music festival"
   Example: "What sold most during the music festival?"
   SQL Strategy: Find events, then join with sales for actual performance

=== DECISION TREE ===
1. Does query mention "sales", "sold", "perform", "revenue", "growing", "traffic"? ‚Üí TYPE 1 or TYPE 2
   - If ALSO mentions "weather affect", "due to weather" ‚Üí TYPE 2 (WDD + SALES combined)
   - If NO weather causation mentioned ‚Üí TYPE 1 (SALES only)
   
2. Does query mention "weather" without "sales" context? ‚Üí TYPE 3 (WDD only)

3. Does query mention "inventory", "stock", "batch"? ‚Üí TYPE 4

4. Does query mention "spoilage", "waste"? ‚Üí TYPE 5

5. Does query mention event names or types? ‚Üí TYPE 6

=== TABLE SCHEMAS ===

METRICS TABLE (Weekly WDD TREND data - NOT actual sales numbers!):
================================================================================
CRITICAL UNDERSTANDING: The metrics table contains TREND VALUES for analyzing 
weather-driven demand spikes or dips. These numbers are NOT actual sales/demand!

- metrics (id, product, location, end_date, metric, metric_nrm, metric_ly)
  * product = Product name (e.g., 'Hamburgers', 'Coffee & Tea', 'Milk')
  * location = Store ID (e.g., 'ST0050', 'ST2900')
  * end_date = Week ending date (joins with calendar.end_date)
  * metric = WDD (Weather Driven Demand) TREND VALUE - weather-adjusted
  * metric_nrm = Normal demand TREND (baseline, no weather impact) - FOR FUTURE ‚â§4 weeks
  * metric_ly = Last Year demand TREND - FOR PAST/YoY/>4 weeks comparisons
  
  IMPORTANT: metric numbers are NOT actual demand - they're TREND VALUES!
  USE metrics table to calculate WDD PERCENTAGE, then apply to actual sales.
  
  WDD FORMULA SELECTION RULES:
  - Short-term (‚â§4 weeks, FUTURE): (metric - metric_nrm) / metric_nrm * 100
  - Long-term (>4 weeks) OR Historical/YoY: (metric - metric_ly) / metric_ly * 100
  
  USE FOR: Weather impact analysis, WDD percentage calculations, demand trend forecasting

SALES TABLE (Actual transaction-level sales data - REAL PERFORMANCE):
- sales (id, batch_id, store_code, product_code, transaction_date, sales_units, sales_amount, discount_amount, total_amount)
  * batch_id = Links to batches.batch_id
  * store_code = Store ID (links to location.location) e.g., 'ST0050'
  * product_code = Product ID INTEGER (links to product_hierarchy.product_id)
  * transaction_date = Date of sale (DATE type)
  * sales_units = Number of units sold (INTEGER) ‚Üê ACTUAL UNITS SOLD
  * sales_amount = Gross sales amount before discount (NUMERIC)
  * discount_amount = Discount applied (NUMERIC)
  * total_amount = Net sales after discount (NUMERIC)
  * REVENUE CALCULATION: SUM(sales_units * total_amount) ‚Üê CRITICAL FORMULA!
  * USE THIS for: actual revenue, units sold, sales performance, "how did X perform" questions

BATCHES TABLE (Inventory batches with expiry tracking):
- batches (id, batch_id, store_code, product_code, transaction_date, expiry_date, unit_price, total_value, received_qty, mfg_date, week_end_date, stock_received, stock_at_week_start, stock_at_week_end)
  * batch_id = Unique batch identifier (PRIMARY KEY for batch queries)
  * store_code = Store ID (links to location.location)
  * product_code = Product ID INTEGER (links to product_hierarchy.product_id)
  * transaction_date = Date batch was received
  * expiry_date = Expiration date (DATE) - use for expiry analysis!
  * mfg_date = Manufacturing date
  * received_qty = Quantity received in batch
  * stock_at_week_start = Opening stock for the week
  * stock_at_week_end = Closing stock for the week
  * week_end_date = Week ending date (joins with calendar.end_date)
  * USE THIS for: inventory levels, expiry tracking, batch lifecycle

BATCH_STOCK_TRACKING TABLE (Inventory movements):
- batch_stock_tracking (record_id, batch_id, store_code, product_code, transaction_type, transaction_date, qty_change, stock_after_transaction, unit_price)
  * record_id = Unique record ID
  * batch_id = Links to batches.batch_id
  * store_code = Store ID (links to location.location)
  * product_code = Product ID INTEGER (links to product_hierarchy.product_id)
  * transaction_type = Movement type:
    - 'SALE' = Units sold from this batch
    - 'TRANSFER_IN' = Stock transferred in
    - 'ADJUSTMENT' = Inventory adjustment
    - 'SPOILAGE' = Stock spoiled/wasted
    - 'RETURN' = Customer returns
  * transaction_date = Date of movement (DATE type)
  * qty_change = Quantity changed (positive=in, negative=out)
  * stock_after_transaction = Running balance after this transaction
  * USE THIS for: stock movement analysis, transfer tracking

SPOILAGE_REPORT TABLE (Waste tracking):
- spoilage_report (id, batch_id, store_code, product_code, qty, spoilage_qty, spoilage_pct, spoilage_case)
  * batch_id = Links to batches.batch_id
  * store_code = Store ID (links to location.location)
  * product_code = Product ID INTEGER (links to product_hierarchy.product_id)
  * qty = Original quantity in batch
  * spoilage_qty = Quantity spoiled (INTEGER)
  * spoilage_pct = Spoilage percentage (0-100, NUMERIC)
  * spoilage_case = Severity level (INTEGER: 1=Low 0-5%, 2=Medium 5-10%, 3=High 10-20%, 4=Critical 20%+)
  * USE THIS for: spoilage analysis, waste patterns, loss tracking

EVENTS TABLE (for event-based analysis - IMPORTANT for festival/holiday queries):
- events (id, event, event_type, event_date, store_id, region, market, state)
  * event = Event name (e.g., 'Memorial Day', 'Black Friday', 'Music Festival')
  * event_type = Category (e.g., 'National Holiday', 'Sporting Event', 'Concert', 'Festival')
  * event_date = Date of event (DATE type)
  * store_id = Affected store (links to location.location)
  * region, market, state = Location info
  * USE THIS for: finding events near stores, event impact on sales

WEEKLY_WEATHER TABLE:
- weekly_weather (id, week_end_date, avg_temp_f, temp_anom_f, tmax_f, tmin_f, 
                  precip_in, precip_anom_in, heatwave_flag, cold_spell_flag, heavy_rain_flag, snow_flag, store_id)
  * week_end_date = Week ending date (joins with calendar.end_date)
  * store_id = Store ID (links to location.location)
  * heatwave_flag, cold_spell_flag, heavy_rain_flag, snow_flag = BOOLEAN weather alerts

LOCATION TABLE:
- location (id, location, region, market, state, latitude, longitude)
  * location = Store ID (PRIMARY KEY, e.g., 'ST6111')

‚ö†Ô∏è CRITICAL: SQL JOIN ORDERING RULES (Prevents "missing FROM-clause entry" errors)

1. **ALWAYS define tables BEFORE referencing them in JOIN conditions**
   ‚ùå WRONG:
   FROM sales s
   JOIN metrics m ON m.product = ph.product  -- ERROR! ph not defined yet
   JOIN product_hierarchy ph ON s.product_code = ph.product_id
   
   ‚úÖ CORRECT:
   FROM sales s
   JOIN product_hierarchy ph ON s.product_code = ph.product_id  -- Define ph first
   JOIN metrics m ON m.product = ph.product  -- Now we can use ph

2. **Recommended JOIN order** (define lookup/dimension tables before fact tables):
   a) Start with main fact table (sales, metrics, batches, etc.)
   b) Join dimension tables next (product_hierarchy, location, calendar)
   c) Join secondary fact tables that reference dimensions (metrics, weather)
   d) Join aggregation/CTE tables last

3. **Example - Correct JOIN order for multi-table queries**:
   ```sql
   FROM sales s
   JOIN product_hierarchy ph ON s.product_code = ph.product_id
   JOIN location l ON s.store_code = l.location
   JOIN calendar c ON s.transaction_date = c.end_date
   JOIN metrics m ON m.product = ph.product 
                  AND m.location = l.location 
                  AND m.end_date = c.end_date
   LEFT JOIN sales s_ly ON s_ly.product_code = s.product_code 
                        AND s_ly.store_code = s.store_code 
                        AND s_ly.transaction_date = c.end_date - INTERVAL '1 year'
   ```

4. **Common error pattern to avoid**:
   - Referencing table alias before it's joined
   - Using columns from tables that appear later in the FROM clause
   - Solution: Reorder JOINs so all referenced tables are defined first
  * region = LOWERCASE: 'northeast', 'southeast', 'midwest', 'west', 'southwest'
  * market = Market area (e.g., 'chicago, il', 'dallas, tx')
  * state = State name lowercase (e.g., 'illinois', 'texas')

PRODUCT_HIERARCHY TABLE (CRITICAL - Supports 3-level hierarchy filtering):
- product_hierarchy (product_id, dept, category, product)
  * product_id = INTEGER PRIMARY KEY (for joining with sales, batches, etc.)
  * dept = Department name (e.g., 'Fast Food', 'Grocery') - HIGHEST LEVEL
  * category = Product category (e.g., 'QSR', 'Perishable', 'Beverages') - MIDDLE LEVEL
  * product = Specific product name (e.g., 'Hamburgers', 'Milk', 'Sandwiches') - LOWEST LEVEL
  
  ‚ö†Ô∏è CRITICAL: "Restaurant Sector" is a special PRODUCT-level entry (no parent hierarchy) for sector-level analysis!
  "Restaurant Sector" or "Restaurant traffic" ‚Üí Use ph.product = 'Restaurant Sector' (NOT ph.category = 'QSR'!)
  
  üîç HIERARCHICAL FILTERING LOGIC:
  
  Level 1 - DEPARTMENT (Broadest):
    - User mentions: "Fast Food", "Grocery", "Department"
    - SQL: WHERE ph.dept = 'dept_name'
    - Example: "Fast Food products" ‚Üí WHERE ph.dept = 'Fast Food'
  
  Level 2 - CATEGORY (Middle):
    - User mentions: "QSR", "Perishable", "Beverages", "Category"
    - SQL: WHERE ph.category = 'category_name'
    - Example: "QSR products" ‚Üí WHERE ph.category = 'QSR'
    - Example: "Perishable items" ‚Üí WHERE ph.category = 'Perishable'
  
  Level 3 - PRODUCT (Most Specific):
    - User mentions specific names: "Hamburgers", "Milk", "Sandwiches"
    - SQL: WHERE ph.product = 'product_name' OR WHERE ph.product IN ('p1', 'p2')
    - Example: "Sandwiches" ‚Üí WHERE ph.product = 'Sandwiches'
    - ‚ö†Ô∏è SPECIAL CASE: "Restaurant Sector" ‚Üí WHERE ph.product = 'Restaurant Sector' (sector-level product, no parent)
  
  üö® CRITICAL EXAMPLES:
  
  ‚úÖ CORRECT - "Which QSR products should perform best?"
     SELECT ph.product FROM product_hierarchy ph WHERE ph.category = 'QSR'
     ‚Üí Returns: Hamburgers, Chicken, Coffee & Tea, Desserts, Sandwiches, Smoothies, Salads, Pizza (8 products)
  
  ‚ùå WRONG - "Which QSR products should perform best?"
     SELECT ... WHERE ph.product IN ('Sandwiches', 'Salads', 'Chicken')
     ‚Üí Missing: Hamburgers, Coffee & Tea, Desserts, Smoothies, Pizza (incomplete!)
  
  ‚úÖ CORRECT - "How many products in QSR category?"
     SELECT COUNT(DISTINCT product) FROM product_hierarchy WHERE category = 'QSR'
     ‚Üí Returns: 8
  
  ‚ùå WRONG - "How many products in QSR category?"
     SELECT COUNT(*) WHERE product IN ('Sandwiches', 'Salads')
     ‚Üí Returns: 2 (should be 8!)
  
  ‚úÖ CORRECT - "What department are perishable items in?"
     SELECT DISTINCT dept FROM product_hierarchy WHERE category = 'Perishable'
     ‚Üí Returns: 'Grocery'
  
  ‚ùå WRONG - "What department are perishable items in?"
     SELECT dept WHERE product IN ('Milk', 'Eggs')
     ‚Üí Misses other perishable products like 'Bacon'!
  
  üìä ACTUAL DATA IN DATABASE:
     Fast Food Department ‚Üí QSR Category ‚Üí 8 products:
       - Hamburgers, Chicken, Coffee & Tea, Desserts, Sandwiches, Smoothies, Salads, Pizza
     
     Grocery Department ‚Üí Perishable Category ‚Üí 3 products in product_hierarchy:
       - Bacon, Eggs, Milk
     
     (More perishable products like Ice Cream are in the perishable table but not in product_hierarchy)
  
  ‚úÖ CORRECT - "Restaurant traffic this Fall"
     SELECT ... WHERE ph.product = 'Restaurant Sector' AND c.season = 'Fall'
     ‚Üí Returns: ALL restaurant products across ALL categories (QSR, Fast Food, Casual Dining, etc.)
  
  ‚ùå WRONG - "Restaurant traffic this Fall"
     SELECT ... WHERE ph.category = 'QSR' AND c.season = 'Fall'
     ‚Üí WRONG! Only returns QSR category, misses other restaurant categories
  
  üìä NOTE: 'Restaurant Sector' is a special product entry added for sector-level analysis (no parent hierarchy)

‚ö†Ô∏è IMPORTANT: NULL Category/Dept Handling (Sector-Level Products)
  Some products have NULL category or dept values - these are VALID sector-level or aggregate products:
  - Examples: 'Restaurant Sector', 'Grocery Sector', 'Home Improvement Sect.', 'Total Fleece', 'Total Shorts', 'Total Boots'
  - These represent sector-level aggregates without detailed hierarchy
  - When selecting category/dept in results: Use COALESCE(ph.category, 'General') AS category
  - Always include in GROUP BY: GROUP BY ph.product, ph.category, ph.dept (even if NULL)
  - DO NOT filter out NULL values - they are valid and indicate general/sector-level products
  - In explanations, clarify that NULL category means "sector-level or general product"

PERISHABLE TABLE (Extended perishable product details):
- perishable (id, product, perishable_id, min_period, max_period, period_metric, storage)
  * product = Perishable product name (e.g., 'Bacon', 'Eggs', 'Milk', 'Ice Cream')
  * min_period, max_period = Shelf life range as TEXT (e.g., '7', '14', '30') - MUST CAST TO INTEGER!
  * period_metric = Unit of time ('Days', 'Weeks', 'Months')
  * storage = Storage requirements ('Refrigerate', 'Freeze', 'Pantry', etc.)
  
  ‚ö†Ô∏è CRITICAL - SHELF LIFE CALCULATIONS:
  When doing arithmetic with max_period, ALWAYS cast to integer first:
  ‚úÖ CORRECT: CAST(p.max_period AS INTEGER) AS shelf_life_days
  ‚úÖ CORRECT: CAST(p.max_period AS INTEGER) - some_numeric_value
  ‚ùå WRONG: p.max_period - some_numeric_value  (TEXT - NUMERIC causes error!)
  ‚ùå WRONG: MAX(p.max_period) AS shelf_life_days  (returns TEXT, not INTEGER!)
  
  üîç PERISHABLE QUERIES:
  - For complete list: JOIN product_hierarchy WITH perishable table
  - For just hierarchy products: WHERE ph.category = 'Perishable'
  - Example: "What perishable products do we have?"
    SELECT DISTINCT p.product, p.storage, p.max_period, p.period_metric
    FROM perishable p
    ‚Üí Returns: Bacon, Eggs, Milk, Ice Cream (from perishable table)
  
  - Example: "What department are perishable items in?"
    SELECT DISTINCT ph.dept FROM product_hierarchy ph WHERE ph.category = 'Perishable'
    ‚Üí Returns: 'Grocery'
  * storage = 'Refrigerate', 'Freeze', 'Pantry'

CALENDAR TABLE (NRF Retail Calendar):
- calendar (id, end_date, year, quarter, month, week, season)
  * end_date = Week ending date (DATE)
  * year = INTEGER (fiscal year)
  * quarter = INTEGER (1-4)
  * month = STRING (FULL NAME: 'January', 'February', etc.) NOT integer!
  * week = NRF retail week number (1-53)
  * season = 'Spring', 'Summer', 'Fall', 'Winter'

‚ö†Ô∏è CRITICAL - CALENDAR DATE FILTERING:
  When filtering by YEAR, QUARTER, or SEASON - use calendar columns directly:
  ‚úÖ CORRECT: WHERE c.year = 2025 OR c.year IN (2023, 2024, 2025)
  ‚úÖ CORRECT: WHERE c.quarter = 3 AND c.year = 2025
  ‚úÖ CORRECT: WHERE c.season = 'Fall' AND c.year = 2025
  ‚ùå WRONG: WHERE c.year = 2025 AND c.end_date BETWEEN '2025-01-01' AND '2025-12-31'
  
  Why: NRF calendar already has the correct date ranges mapped to years/quarters/seasons!
  You do NOT need to add extra BETWEEN date filters when using calendar.year/quarter/season.
  
  Exception: Only use BETWEEN dates when user specifies exact dates like "March 18 to August 2"

=== JOIN PATTERNS ===
- metrics ‚Üí location: metrics.location = location.location
- metrics ‚Üí product_hierarchy: metrics.product = product_hierarchy.product
- metrics ‚Üí calendar: metrics.end_date = calendar.end_date
- sales ‚Üí location: sales.store_code = location.location
- sales ‚Üí product_hierarchy: sales.product_code = product_hierarchy.product_id
- batches ‚Üí location: batches.store_code = location.location
- batches ‚Üí product_hierarchy: batches.product_code = product_hierarchy.product_id
- batches ‚Üí calendar: batches.week_end_date = calendar.end_date
- batch_stock_tracking ‚Üí batches: batch_stock_tracking.batch_id = batches.batch_id
- spoilage_report ‚Üí batches: spoilage_report.batch_id = batches.batch_id
- spoilage_report ‚Üí product_hierarchy: spoilage_report.product_code = product_hierarchy.product_id
- events ‚Üí location: events.store_id = location.location
- weekly_weather ‚Üí location: weekly_weather.store_id = location.location

=== EVENT-BASED ANALYSIS APPROACH ===
When user asks about events (festivals, holidays, concerts, games):
1. Find relevant events from events table by name/type/date
2. Get the stores affected (events.store_id)
3. For sales impact: Join with sales table on store_code and nearby transaction_date
4. For demand forecast: Join with metrics on location and end_date
5. Look at historical data from same event last year for comparison

Example: "What products should I stock for a music festival in Nashville?"
1. SELECT * FROM events WHERE event ILIKE '%music%' AND market ILIKE '%nashville%'
2. Find affected stores
3. Join with historical sales data around similar events
4. Identify top-selling products during past events""")
        
        # WDD Formulas section
        prompt_parts.append("""
=== WDD FORMULAS (For metrics table ONLY) ===

CRITICAL: Determine if the query is asking about FUTURE or HISTORICAL/PAST data:

1. FUTURE QUERIES (Next week, next month, next 2 weeks, upcoming period, "this coming"):
   Use: WDD vs Normal (metric vs metric_nrm)
   Formula: (SUM(metric) - SUM(metric_nrm)) / NULLIF(SUM(metric_nrm), 0)
   Keywords: "next", "upcoming", "expected", "forecast", "will be", "coming", "planning for"

2. HISTORICAL/PAST QUERIES (Last week, last month, past 6 weeks, previous period):
   Use: WDD vs Last Year (metric vs metric_ly)
   Formula: (SUM(metric) - SUM(metric_ly)) / NULLIF(SUM(metric_ly), 0)
   Keywords: "last", "past", "previous", "ago", "was", "were", "had"

3. YEAR-OVER-YEAR COMPARISONS (Trend analysis, change over time):
   Use: WDD vs Last Year (metric vs metric_ly)
   Formula: (SUM(metric) - SUM(metric_ly)) / NULLIF(SUM(metric_ly), 0)
   Keywords: "year-over-year", "YoY", "compared to last year", "vs last year", "biggest change"

4. PLANNING QUERIES WITH HISTORICAL CONTEXT ("based on last year", "considering last spring"):
   These queries want BOTH historical data AND current planning:
   - First analyze last year's patterns using metric_ly
   - Then show current year forecast vs historical using metric vs metric_ly
   Example: "Considering the impact of weather last spring, how should we plan for this coming spring?"
   ‚Üí This needs YoY comparison to show how current forecast differs from last year
   ‚Üí Use metric vs metric_ly to show year-over-year weather impact differences
   
5. RISK ANALYSIS ("weather-driven risks", "biggest risks"):
   Use: WDD vs Last Year (metric vs metric_ly) 
   A positive % = demand UP vs last year (opportunity)
   A negative % = demand DOWN vs last year (risk if inventory not adjusted)
   Formula: ROUND((SUM(metric) - SUM(metric_ly)) / NULLIF(SUM(metric_ly), 0) * 100, 2) AS yoy_risk_pct

EXAMPLES:
- "next month" ‚Üí Use metric vs metric_nrm
- "last week" ‚Üí Use metric vs metric_ly
- "past 6 weeks" ‚Üí Use metric vs metric_ly
- "biggest change" ‚Üí Use metric vs metric_ly
- "over the last three weeks" ‚Üí Use metric vs metric_ly
- "compared to last year" / "year-over-year" ‚Üí Use metric vs metric_ly
- "considering last spring, plan for this spring" ‚Üí Use metric vs metric_ly (YoY planning)
- "weather-driven risks" ‚Üí Use metric vs metric_ly (negative values = risk)

SEASONS: Spring=Feb/Mar/Apr, Summer=May/Jun/Jul, Fall=Aug/Sep/Oct, Winter=Nov/Dec/Jan

‚ö†Ô∏è CRITICAL - SEASONAL QUERIES (Q6, Q7, Q8, Q9):
==================================================
TEMPORAL MAPPING (Current date: November 8, 2025):
- "last spring" = Spring 2025 (Feb/Mar/Apr 2025) ‚Üí HISTORICAL, c.year = 2025, m.end_date <= '2025-11-08'
- "coming spring" = Spring 2026 (Feb/Mar/Apr 2026) ‚Üí FUTURE, c.year = 2026, m.end_date >= '2025-11-09'
- "past summer" = Summer 2025 (May/Jun/Jul 2025) ‚Üí HISTORICAL, c.year = 2025, m.end_date <= '2025-11-08'
- "coming winter" = Winter 2025-26 (Nov/Dec 2025 + Jan 2026) ‚Üí FUTURE/CURRENT, c.year = 2025, m.end_date >= '2025-11-09'

CRITICAL RULES FOR SEASONAL QUERIES:
1. ‚ùå NEVER use c.year IN (2024, 2025) - causes double-counting! metric already has this year, metric_ly has last year
2. ‚úÖ Filter ONE year only (c.year = 2025 for historical OR c.year = 2026 for future spring/summer/fall)
3. ‚úÖ metric = current year's data, metric_ly = last year's data AUTOMATICALLY in the same row
4. ‚úÖ Historical: Add m.end_date <= '2025-11-08' filter
5. ‚úÖ Future: Add m.end_date >= '2025-11-09' filter
6. ‚úÖ "biggest risks" ‚Üí ORDER BY wdd_pct DESC (highest first)
7. ‚ö†Ô∏è NEVER "ORDER BY ABS(alias)" - PostgreSQL error! Use "ORDER BY ABS((full_calculation))" or just "ORDER BY alias DESC"
8. ‚ùå NEVER ORDER BY ASC for "biggest risks" - that shows SMALLEST changes!
9. ‚úÖ Product hierarchy: ph.dept for sectors (e.g., 'Apparel'), ph.category for subcategories, ph.product for items
10. ‚úÖ Do NOT group by c.month unless explicitly asked - group by season or region only

EXAMPLE CORRECT QUERIES:
- "Allergy Relief last spring" ‚Üí c.season = 'Spring' AND c.year = 2025 AND m.end_date <= '2025-11-08'
- "Boots coming winter risks" ‚Üí c.season = 'Winter' AND c.year = 2025 AND m.end_date >= '2025-11-09' ORDER BY wdd_pct DESC (NOT ABS(alias)!)
- "Apparel past summer vs prior summer" ‚Üí c.season = 'Summer' AND c.year = 2025 AND m.end_date <= '2025-11-08' (metric vs metric_ly shows 2025 vs 2024)
""")
        
        # SQL generation instructions
        prompt_parts.append("""
IMPORTANT SQL GENERATION RULES:
1. Use PostgreSQL syntax (LIMIT not TOP, || for concat)
2. FIRST determine query type (see QUERY TYPE DETECTION above)

=== QUERY TYPE EXAMPLES (Study These!) ===

**TYPE 1 - SALES ONLY** (Use sales table):
Q: "How did the sales of Cleaning Supplies change in Chicago during cold spells in February 2024?"
‚Üí Use: sales table + weekly_weather (join on cold_spell_flag = true)
‚Üí CRITICAL: User said "Cleaning Supplies" (specific product), so filter by product name:
‚Üí WHERE ph.product = 'Cleaning Supplies' (NOT by category!)
‚Üí SELECT ph.product, SUM(s.sales_units), SUM(s.sales_units * s.total_amount) as revenue FROM sales s...

Q: "How did Smoothies perform in Charlotte, NC during heatwave weeks in 2025?"
‚Üí Use: sales table + weekly_weather (join on heatwave_flag = true)
‚Üí CRITICAL: User said "Smoothies" (specific product)
‚Üí Filter: WHERE ph.product = 'Smoothies' (NOT WHERE ph.category = 'QSR'!)
‚Üí Focus on actual units sold and revenue for SMOOTHIES ONLY

Q: "Which region had the fastest growing demand for salad kits between 2023 and 2025?"
‚Üí Use: sales table with calendar.year for NRF fiscal year grouping
‚Üí CRITICAL: Use calendar.year IN (2023, 2024, 2025) - NRF calendar handles the date ranges!
‚Üí Do NOT add extra BETWEEN date filters when using calendar.year!
‚Üí Calculate growth from 2023 to 2025 using CASE WHEN or FILTER clause
‚Üí Example SQL:
   SELECT l.region,
          SUM(CASE WHEN c.year = 2025 THEN s.sales_units ELSE 0 END) AS units_2025,
          SUM(CASE WHEN c.year = 2023 THEN s.sales_units ELSE 0 END) AS units_2023,
          ROUND((SUM(CASE WHEN c.year = 2025 THEN s.sales_units ELSE 0 END) - 
                 SUM(CASE WHEN c.year = 2023 THEN s.sales_units ELSE 0 END)) /
                NULLIF(SUM(CASE WHEN c.year = 2023 THEN s.sales_units ELSE 0 END), 0) * 100, 2) AS growth_pct
   FROM sales s
   JOIN product_hierarchy ph ON s.product_code = ph.product_id
   JOIN location l ON s.store_code = l.location
   JOIN calendar c ON s.transaction_date = c.end_date
   WHERE ph.product = 'Salad Kits'
     AND c.year IN (2023, 2025)  -- Only the years needed for comparison
   GROUP BY l.region
   HAVING SUM(CASE WHEN c.year = 2023 THEN s.sales_units ELSE 0 END) > 0  -- Ensure 2023 baseline exists
   ORDER BY growth_pct DESC;

**TYPE 2 - WDD + SALES COMBINED** (Join metrics AND sales):
Q: "Sandwich sales were down in Southeast last month. How much was due to weather?"
‚Üí Use: metrics (for WDD trend) + sales (for actual sales performance)
‚Üí Compare WDD change (metric vs metric_ly) against actual sales change
‚Üí Example SQL:
   SELECT 
       ph.product,
       SUM(s.sales_units) as actual_units,
       SUM(s.sales_units) - SUM(s_ly.sales_units) as sales_change,
       ROUND((SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100, 2) as wdd_change_pct
   FROM sales s
   JOIN metrics m ON m.product = ph.product AND m.location = s.store_code AND m.end_date = ...
   JOIN sales s_ly ON ... (last year data)
   WHERE...

Q: "How much did weather affect demand in Apparel sector this past summer compared to prior summer?"
‚Üí Use: metrics (WDD trends) + sales (actual sales)
‚Üí Show both weather impact % AND actual sales difference
‚Üí Example SQL:
   SELECT 
       ph.category,
       SUM(s.sales_units) as summer_2025_units,
       SUM(s_ly.sales_units) as summer_2024_units,
       ROUND((SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100, 2) as wdd_impact_pct
   FROM sales s
   JOIN metrics m ON m.product = ph.product AND m.location = s.store_code AND m.end_date = s.transaction_date
   JOIN product_hierarchy ph ON s.product_code = ph.product_id
   WHERE ph.category = 'Apparel' AND c.month IN ('May', 'June', 'July')

Q: "Which markets had the most favorable weather impacts on restaurant traffic this Fall compared to last Fall?"
‚Üí Use: metrics (WDD) + sales (actual traffic = sales_units)
‚Üí Join both tables to show weather correlation with actual sales
‚Üí Example SQL:
   SELECT 
       l.market,
       SUM(s.sales_units) as actual_traffic,
       ROUND((SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100, 2) as wdd_impact_pct
   FROM sales s
   JOIN metrics m ON m.product = ph.product AND m.location = s.store_code
   JOIN location l ON s.store_code = l.store_id
   WHERE c.season = 'Fall' AND c.year = 2025

Q: "Based on the weather, what grocery products had the best year-over-year performance last quarter?"
‚Üí Use: metrics (WDD trends) + sales (actual performance)
‚Üí Filter by positive WDD impact, rank by sales YoY growth
‚Üí Example SQL:
   SELECT 
       ph.product,
       SUM(s.sales_units) as current_qtr_units,
       SUM(s_ly.sales_units) as last_year_qtr_units,
       ROUND((SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100, 2) as wdd_impact_pct
   FROM sales s
   JOIN metrics m ON m.product = ph.product AND m.location = s.store_code
   JOIN product_hierarchy ph ON s.product_code = ph.product_id
   WHERE ph.dept = 'Grocery' AND wdd_impact_pct > 0
   ORDER BY (current_qtr_units - last_year_qtr_units) DESC

Q: "Considering the impact of weather last spring, how should we plan for allergy relief this coming spring?"
‚Üí Use: metrics (WDD trends with metric_ly for year-over-year comparison)
‚Üí This is a PLANNING query with historical reference - use metric vs metric_ly!
‚Üí Filter by product, season='Spring', show YoY WDD change
‚Üí Example SQL:
   SELECT 
       ph.product,
       l.region,
       c.month,
       SUM(m.metric) as current_forecast,
       SUM(m.metric_ly) as last_year_demand,
       ROUND((SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100, 2) as yoy_change_pct,
       ROUND(SUM(m.metric_ly) * (1 + (SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0)), 0) as recommended_order
   FROM metrics m
   JOIN product_hierarchy ph ON m.product = ph.product
   JOIN location l ON m.location = l.location
   JOIN calendar c ON m.end_date = c.end_date
   WHERE ph.product = 'Allergy Relief'
     AND c.season = 'Spring'
     AND c.year = 2026
   GROUP BY ph.product, l.region, c.month
   ORDER BY l.region, c.month

**TYPE 3 - WDD ONLY** (Use metrics table only):
Q: "What is the weather-driven demand forecast for ice cream?"
‚Üí Use: metrics table only (trend analysis)
‚Üí No sales join needed

**TYPE 6 - EVENT-BASED** (Use events + sales):
Q: "Which Non-Perishable products sold the most during the Los Angeles Lakers VS Golden State Warriors game week?"
‚Üí First: Find event in events table
‚Üí Then: Join with sales for actual sales during that week
‚Üí Focus on sales_units from sales table

=== TABLE USAGE RULES ===
3. For WEATHER DEMAND analysis ‚Üí Use 'metrics' table
4. For ACTUAL SALES data ‚Üí Use 'sales' table (NOT metrics!)
5. For WDD + SALES COMBINED ‚Üí JOIN both metrics and sales tables
6. For INVENTORY/STOCK ‚Üí Use 'batches' and 'batch_stock_tracking' tables
7. For SPOILAGE/WASTE ‚Üí Use 'spoilage_report' table
8. For EVENT analysis ‚Üí Use 'events' table, join with sales for performance
   üö® CRITICAL: Search BOTH e.event AND e.event_type columns!
   Example: WHERE (e.event ILIKE '%festival%' OR e.event_type ILIKE '%festival%')

üö® CRITICAL - PRODUCT vs CATEGORY FILTERING:
- When user mentions SPECIFIC PRODUCT NAMES (e.g., "Sandwiches", "Smoothies", "Salad Kits"):
  ‚úÖ CORRECT: WHERE ph.product = 'Sandwiches' OR WHERE ph.product IN ('Sandwiches', 'Tomatoes', 'Lettuce')
  ‚ùå WRONG: WHERE ph.category = 'QSR' (this returns ALL QSR products, not just Sandwiches!)
  
- üö®üö® ONLY filter by the EXACT products the user mentioned - DO NOT add ANY other products:
  User says: "Sandwiches, tomatoes, lettuce" (3 products)
  ‚úÖ CORRECT: WHERE ph.product IN ('Sandwiches', 'Tomatoes', 'Lettuce')  -- Exactly 3 products
  ‚ùå WRONG: WHERE ph.product IN ('Sandwiches', 'Tomatoes', 'Lettuce', 'Salads', 'Salad Kits')  -- 5 products!
  ‚ùå WRONG: WHERE ph.category IN ('QSR', 'Perishable')  -- Would return 16 products!
  
- The number of products in your WHERE clause MUST match the number of products user mentioned!
  User mentions 3 products ‚Üí WHERE clause must have EXACTLY 3 products
  User mentions 1 product ‚Üí WHERE clause must have EXACTLY 1 product
  
- Only filter by category when user asks about the CATEGORY itself:
  ‚úÖ CORRECT: "Show me all QSR products" ‚Üí WHERE ph.category = 'QSR'
  ‚úÖ CORRECT: "Perishable products" ‚Üí WHERE ph.category = 'Perishable'
  
- For related products mentioned explicitly by user:
  ‚úÖ CORRECT: "demand for tomatoes, lettuce" ‚Üí WHERE ph.product IN ('Tomatoes', 'Lettuce')
  ‚ùå WRONG: WHERE ph.category = 'Perishable' (returns 8 products instead of 2!)

CRITICAL - REVENUE CALCULATION:
- Total Sale Units = SUM(sales_units)
- Revenues = SUM(sales_units * total_amount)  ‚Üê ALWAYS USE THIS FORMULA!
- NEVER use SUM(total_amount) alone for revenue
- Example: SELECT product, SUM(sales_units) as units, SUM(sales_units * total_amount) as revenue FROM sales...

üö® CRITICAL - WEATHER FLAG FILTERING:
- When user mentions "heatwave", "heat wave", or "hot weather" ‚Üí MUST filter: WHERE heatwave_flag = true
- When user mentions "cold spell", "cold weather", "freezing" ‚Üí MUST filter: WHERE cold_spell_flag = true  
- When user mentions "heavy rain", "rainy", "precipitation" ‚Üí MUST filter: WHERE heavy_rain_flag = true
- When user mentions "snow", "snowy" ‚Üí MUST filter: WHERE snow_flag = true
- Join weekly_weather: JOIN weekly_weather w ON w.week_end_date = c.end_date AND w.store_id = l.location
- CRITICAL: weekly_weather uses week_end_date NOT end_date!
- Example WRONG: JOIN weekly_weather w ON w.end_date = c.end_date ‚ùå (column doesn't exist!)
- Example CORRECT: JOIN weekly_weather w ON w.week_end_date = c.end_date ‚úÖ
- Example CORRECT: "heatwave impact" ‚Üí WHERE w.heatwave_flag = true ‚úÖ

üö® CRITICAL - COMPARISON QUERIES ("vs" or "compared to"):
- When user asks "heatwave vs normal", "cold spell vs normal", "event weeks vs non-event" ‚Üí Use CASE statements!
- Calculate BOTH conditions separately in same query:
  SUM(CASE WHEN w.heatwave_flag = true THEN s.sales_units ELSE 0 END) AS heatwave_sales,
  SUM(CASE WHEN w.heatwave_flag = false THEN s.sales_units ELSE 0 END) AS normal_sales
- This allows direct comparison in one result set
- Example: "Smoothies during heatwave vs normal weeks" ‚Üí Use CASE to split by heatwave_flag
- REMEMBER: JOIN weekly_weather w ON w.week_end_date = c.end_date (NOT w.end_date!)

CRITICAL - ENTITY VALIDATION:
- Valid regions ONLY: 'northeast', 'southeast', 'midwest', 'west', 'southwest' (all lowercase)
- If user mentions invalid region (e.g., "Midwest" with capital M, "pacific", "central"), query will return 0 rows
- Check events table for actual events - if event doesn't exist, SQL returns 0 rows (this is CORRECT)
- Never fabricate data - 0 rows means NO DATA EXISTS for that criteria

CRITICAL JOIN RULES:
- sales/batches/spoilage use product_code (INTEGER) ‚Üí joins with product_hierarchy.product_id
- metrics uses product (STRING) ‚Üí joins with product_hierarchy.product
- All inventory tables use store_code ‚Üí joins with location.location

8. For REGION queries: l.region = 'northeast' (LOWERCASE!)
9. calendar.month is STRING ('January', 'December') NOT integer!
10. ALWAYS use NULLIF(denominator, 0) to prevent division by zero
11. Use SELECT DISTINCT unless aggregating with GROUP BY
12. Maximum 30 rows (use LIMIT 30)
13. Return ONLY the SQL query, no explanation

=== SPECIAL CASES ===

**"Ideal Beach Weather" / "Perfect Weather" Queries:**
Q: "How should food options be diversified for peak weekend sales driven by ideal beach weather in Miami?"
‚Üí Strategy:
   1. Find weeks with "ideal beach weather" using weekly_weather table:
      - Temperature: tmax_f BETWEEN 75 AND 90 (warm but not extreme)
      - Low precipitation: precip_in < 0.3 (minimal rain - average is 0.59)
      - No extreme flags: heatwave_flag = false, heavy_rain_flag = false
   2. Join with sales table to see what sold best during those ideal weeks
   3. Filter by Miami market (location.market ILIKE '%miami%')
   4. Group by product category to see diversification opportunities
   5. Include week-over-week comparison for trend analysis

Example SQL:
SELECT ph.category, ph.product, 
       SUM(s.sales_units) as total_units,
       SUM(s.sales_units * s.total_amount) as revenue
FROM sales s
JOIN product_hierarchy ph ON s.product_code = ph.product_id
JOIN location l ON s.store_code = l.location
JOIN calendar c ON s.transaction_date = c.end_date
JOIN weekly_weather w ON w.week_end_date = c.end_date AND w.store_id = l.location
WHERE l.market ILIKE '%miami%'
  AND w.tmax_f BETWEEN 75 AND 90
  AND w.precip_in < 0.3  -- Low precipitation = ideal beach weather
  AND w.heatwave_flag = false
  AND w.heavy_rain_flag = false
GROUP BY ph.category, ph.product
ORDER BY total_units DESC
LIMIT 20;

**"Cold Spell / Heatwave Impact + Ordering Recommendations" Queries:**
Q: "Our store is in Northeast expecting a cold spell. What is the decrease in demand for Salad Kits and how to adjust ordering?"
‚Üí Strategy:
   1. Use metrics table to get WDD trend during cold spells (to show weather impact)
   2. Filter by SPECIFIC product mentioned (Salad Kits) - NOT all products!
   3. Filter by region (northeast)
   4. Calculate demand change using metric vs metric_nrm (short-term) - this shows the PERCENTAGE change
   5. For ordering recommendations: Use the WDD % to adjust ACTUAL last-week sales

Example SQL (WDD percentage analysis):
SELECT ph.product, l.region,
       ROUND((SUM(m.metric) - SUM(m.metric_nrm)) / NULLIF(SUM(m.metric_nrm), 0) * 100, 2) AS demand_change_pct,
       SUM(m.metric) as forecasted_trend,
       SUM(m.metric_nrm) as normal_trend
FROM metrics m
JOIN product_hierarchy ph ON m.product = ph.product
JOIN location l ON m.location = l.location
JOIN calendar c ON m.end_date = c.end_date
JOIN weekly_weather w ON w.week_end_date = c.end_date AND w.store_id = l.location
WHERE ph.product = 'Salad Kits'  -- CRITICAL: Filter by EXACT product mentioned!
  AND l.region = 'northeast'
  AND w.cold_spell_flag = true
GROUP BY ph.product, l.region
LIMIT 30;

-- NOTE: For recommended ordering quantity, use the Adjusted Qty formula:
-- Recommended Qty = Last-week ACTUAL sales √ó (1 + WDD %)
-- Get last-week sales from sales table, NOT from metrics!

**"Rise in Sales Without Weather/Event" Queries (Anomaly Detection - Q2):**
Q: "Alert me to products in Columbia, SC that experienced a rise in sales but no weather or event recorded"

‚ö†Ô∏è CRITICAL REQUIREMENTS:
   1. **Use SALES table** (NOT metrics) - Need actual sales_units, revenue, transaction data
   2. **Quarter-on-Quarter (QoQ) comparison** - Last 8 quarters using LAG window function
   3. **Calculate quarterly totals FIRST** - Don't filter during aggregation
   4. **Check weather/events SEPARATELY** - Use separate CTEs to identify quarters with weather/events
   5. **Filter at the end** - Show only increases where had_weather = false AND had_event = false
   6. **Date range**: '2023-11-08' to '2025-11-08' (8 quarters = 2 years)
   7. **Output format**: product, qoq_pct, prev_units ‚Üí curr_units, prev_quarter ‚Üí curr_quarter, revenue
   
‚ö†Ô∏è CRITICAL FIXES:
   - Revenue: Use `SUM(s.sales_units * s.total_amount)` - This is the CORRECT formula!
   - Calendar join: `s.transaction_date = c.end_date` (sales.transaction_date is already week-ending)
   - Events join: Use `e.store_id = l.location` (events table has store_id, location table has location)
   - Location table: Uses column name `location` (not `store_code` for FK reference)
   - Don't filter weather/events during quarterly aggregation - check separately then filter results
   
‚ö†Ô∏è CRITICAL - Weather/Event Check Logic:
   - Create separate CTEs to check if quarters had weather flags or events
   - Use BOOL_OR to check if ANY week in the quarter had adverse weather
   - LEFT JOIN these checks to quarterly_sales
   - Filter final results where both had_weather = false AND had_event = false
   
‚ö†Ô∏è BUSINESS CONTEXT:
   - **Goal**: Find unexplained sales anomalies (increases without external triggers)
   - **Why**: Identifies opportunities, competitive wins, or demographic shifts needing investigation
   - **8 quarters**: Provides enough history to see meaningful QoQ trends
   
CORRECT QUERY TEMPLATE (Q2):
```sql
WITH quarterly_sales AS (
    SELECT ph.product,
           c.quarter, c.year,
           SUM(s.sales_units) AS total_units,
           SUM(s.sales_units * s.total_amount) AS total_revenue,  -- Correct revenue formula!
           'Q' || c.quarter || '-' || SUBSTRING(c.year::TEXT, 3, 2) AS quarter_label
    FROM sales s
    JOIN product_hierarchy ph ON s.product_code = ph.product_id
    JOIN location l ON s.store_code = l.location  -- location table uses 'location' column
    JOIN calendar c ON s.transaction_date = c.end_date  -- Direct join on transaction_date
    WHERE l.market ILIKE '%columbia, sc%'
      AND c.end_date BETWEEN '2023-11-08' AND '2025-11-08'  -- Last 8 quarters (2 years)
    GROUP BY ph.product, c.quarter, c.year
),
weather_check AS (
    -- Check which quarters had adverse weather
    SELECT c.quarter, c.year,
           BOOL_OR(ww.heatwave_flag OR ww.cold_spell_flag OR ww.heavy_rain_flag OR ww.snow_flag) AS had_weather
    FROM calendar c
    JOIN weekly_weather ww ON ww.week_end_date = c.end_date
    JOIN location l ON ww.store_id = l.location
    WHERE l.market ILIKE '%columbia, sc%'
      AND c.end_date BETWEEN '2023-11-08' AND '2025-11-08'
    GROUP BY c.quarter, c.year
),
event_check AS (
    -- Check which quarters had events (events table has store_id)
    SELECT c.quarter, c.year,
           COUNT(e.event) > 0 AS had_event
    FROM calendar c
    CROSS JOIN location l
    LEFT JOIN events e ON e.store_id = l.location  -- events.store_id joins to location.location
                       AND e.event_date BETWEEN c.end_date - INTERVAL '7 days' AND c.end_date
    WHERE l.market ILIKE '%columbia, sc%'
      AND c.end_date BETWEEN '2023-11-08' AND '2025-11-08'
    GROUP BY c.quarter, c.year
),
lagged_sales AS (
    SELECT qs.*,
           LAG(qs.total_units) OVER (PARTITION BY qs.product ORDER BY qs.year, qs.quarter) AS prev_quarter_units,
           LAG(qs.quarter_label) OVER (PARTITION BY qs.product ORDER BY qs.year, qs.quarter) AS prev_quarter_label,
           COALESCE(wc.had_weather, false) AS had_weather,
           COALESCE(ec.had_event, false) AS had_event
    FROM quarterly_sales qs
    LEFT JOIN weather_check wc ON qs.quarter = wc.quarter AND qs.year = wc.year
    LEFT JOIN event_check ec ON qs.quarter = ec.quarter AND qs.year = ec.year
)
SELECT product,
       ROUND(((total_units - prev_quarter_units)::NUMERIC / NULLIF(prev_quarter_units, 0)) * 100, 2) AS qoq_pct,
       prev_quarter_units,
       total_units AS curr_quarter_units,
       total_revenue,
       prev_quarter_label,
       quarter_label AS curr_quarter_label
FROM lagged_sales
WHERE prev_quarter_units IS NOT NULL
  AND total_units > prev_quarter_units  -- Only increases
  AND had_weather = false  -- No adverse weather
  AND had_event = false    -- No events
ORDER BY qoq_pct DESC
LIMIT 30;
```

**Q2 Context - Unexplained Sales Anomalies**:
- **Goal**: Find products with sales increases that happened WITHOUT external factors
- **Time Period**: Last 8 quarters (2 years) from 2023-11-08 to 2025-11-08
- **"Unexplained" means**: No heatwave, cold spell, heavy rain, snow flags AND no events nearby
- **Output**: Product name, QoQ %, prev ‚Üí current units, prev ‚Üí current quarter, revenue
- **Business Value**: Identify opportunities or issues requiring investigation

**"Weather Impact + Stockout Risk" Queries (Short-term Forecast + Inventory Risk - Q12):**
Q: "How could expected weather impact product demand in next 1-2 weeks, and which items need replenishment to avoid stockouts?"

‚ö†Ô∏è CRITICAL - AVOID RESTRICTIVE FILTERS:
   This query MUST NOT include product or location filters in WHERE clauses!
   The goal is to find ALL products at risk across ALL stores.
   Entity resolution provides context but should NOT become WHERE clause filters.
   
‚ö†Ô∏è CRITICAL REQUIREMENTS:
   1. **Use BOTH metrics table (WDD forecast) AND batches table (current stock)**
   2. **WDD vs Normal** - Short-term forecast for next 1-2 weeks (metric vs metric_nrm)
   3. **Next weeks dates**: From current 2025-11-08 ‚Üí Next weeks are 2025-11-15, 2025-11-22
   4. **Average weekly sales**: Last 4 weeks (2025-10-12 to 2025-11-08) from sales table
   5. **Current stock**: stock_at_week_end from batches table for current week (2025-11-08)
   6. **Weeks of Cover (WOC)** = current_stock / avg_weekly_sales
   7. **Risk levels**: HIGH < 1 week, MEDIUM 1-2 weeks, LOW ‚â• 2 weeks
   8. **Output**: Product, WDD uplift %, current stock, avg weekly sales, WOC, risk level, risk priority
   9. **Filter**: Only products with current_stock > 0, ordered by risk priority (1=High, 2=Medium, 3=Low)
   10. **DO NOT add product filters (ph.product IN ...)** - Let query find all products with data
   11. **DO NOT add location filters (l.location IN ...)** - Aggregate across all stores
   12. **Group by product only** - Not by market or location (too granular)

‚ö†Ô∏è CRITICAL FORMULAS:
   - **WDD Forecast**: `ROUND((SUM(m.metric) - SUM(m.metric_nrm)) / NULLIF(SUM(m.metric_nrm), 0) * 100, 2)`
   - **Avg Weekly Sales**: `AVG(s.sales_units)` across last 4 weeks (2025-10-12 to 2025-11-08)
   - **Weeks of Cover**: `current_stock / NULLIF(avg_weekly_sales, 0)`
   - **Risk Level**: 
     * CASE WHEN woc < 1 THEN 'HIGH RISK'
            WHEN woc < 2 THEN 'MEDIUM RISK'
            ELSE 'LOW RISK' END
   - **Risk Priority**: 
     * CASE WHEN woc < 1 THEN 1
            WHEN woc < 2 THEN 2
            ELSE 3 END

‚ö†Ô∏è CRITICAL - DO NOT ADD PRODUCT OR LOCATION FILTERS:
   - ‚ùå DO NOT use: WHERE ph.product IN (...)
   - ‚ùå DO NOT use: WHERE l.location IN (...)
   - ‚ùå DO NOT use: WHERE s.store_code IN (...)
   - ‚ùå DO NOT use: WHERE b.store_code IN (...)
   - ‚úÖ CORRECT: Let the query find ALL products with available data
   - ‚úÖ CORRECT: Aggregate across ALL stores for product-level view
   - **Reason**: Product/location filters cause 0 rows - data exists but filters are too restrictive

‚ö†Ô∏è CRITICAL - NULL Category Handling:
   - Some products have NULL category/dept (sector-level products like 'Restaurant Sector', 'Total Fleece')
   - Use COALESCE(ph.category, 'General') to show 'General' instead of NULL
   - This is NORMAL for sector-level aggregate products

CORRECT QUERY TEMPLATE (Q12):
```sql
WITH wdd_forecast AS (
    -- Step 1: WDD forecast for next 1-2 weeks
    SELECT 
        ph.product,
        ph.category,
        ROUND((SUM(m.metric) - SUM(m.metric_nrm)) / NULLIF(SUM(m.metric_nrm), 0) * 100, 2) AS wdd_uplift_pct,
        COUNT(DISTINCT l.market) AS markets_affected
    FROM metrics m
    JOIN product_hierarchy ph ON m.product = ph.product
    JOIN location l ON m.location = l.location
    JOIN calendar c ON m.end_date = c.end_date
    WHERE c.end_date IN ('2025-11-15', '2025-11-22')  -- Next 1-2 weeks
    GROUP BY ph.product, ph.category
    HAVING SUM(m.metric_nrm) > 0  -- Ensure valid baseline
),
avg_weekly_sales AS (
    -- Step 2: Average weekly sales for last 4 weeks
    SELECT 
        ph.product,
        AVG(s.sales_units) AS avg_weekly_sales,
        SUM(s.sales_units) AS total_sales_4wks
    FROM sales s
    JOIN product_hierarchy ph ON s.product_code = ph.product_id
    JOIN calendar c ON s.transaction_date = c.end_date
    WHERE c.end_date BETWEEN '2025-10-12' AND '2025-11-08'  -- Last 4 weeks
    GROUP BY ph.product
    HAVING SUM(s.sales_units) > 0  -- Only products with sales
),
current_stock AS (
    -- Step 3: Current stock as of 2025-11-08
    SELECT 
        ph.product,
        SUM(b.stock_at_week_end) AS current_stock
    FROM batches b
    JOIN product_hierarchy ph ON b.product_code = ph.product_id
    WHERE b.week_end_date = '2025-11-08'
      AND b.stock_at_week_end > 0  -- Only products with inventory
    GROUP BY ph.product
)
-- Final: Combine with LEFT JOINs to show partial data
SELECT 
    COALESCE(wf.product, aws.product, cs.product) AS product,
    COALESCE(wf.category, 'General') AS category,  -- Handle NULL categories
    wf.wdd_uplift_pct,
    wf.markets_affected,
    cs.current_stock,
    aws.avg_weekly_sales,
    ROUND(cs.current_stock / NULLIF(aws.avg_weekly_sales, 0), 2) AS weeks_of_cover,
    CASE 
        WHEN cs.current_stock / NULLIF(aws.avg_weekly_sales, 0) < 1 THEN 'HIGH RISK'
        WHEN cs.current_stock / NULLIF(aws.avg_weekly_sales, 0) < 2 THEN 'MEDIUM RISK'
        ELSE 'LOW RISK'
    END AS risk_level,
    CASE 
        WHEN cs.current_stock / NULLIF(aws.avg_weekly_sales, 0) < 1 THEN 1
        WHEN cs.current_stock / NULLIF(aws.avg_weekly_sales, 0) < 2 THEN 2
        ELSE 3
    END AS risk_priority
FROM wdd_forecast wf
LEFT JOIN current_stock cs ON wf.product = cs.product
LEFT JOIN avg_weekly_sales aws ON wf.product = aws.product
WHERE cs.current_stock > 0 AND aws.avg_weekly_sales > 0  -- Must have both stock and sales
ORDER BY risk_priority ASC, wf.wdd_uplift_pct DESC
LIMIT 30;
```

**Q12 Context - Weather Impact + Stockout Risk**:
- **Goal**: Identify products needing replenishment to avoid stockouts based on weather-driven demand
- **Time Period**: Next 1-2 weeks (2025-11-15, 2025-11-22) for forecast
- **Historical Period**: Last 4 weeks (2025-10-12 to 2025-11-08) for avg weekly sales
- **Risk Assessment**: Weeks of Cover (WOC) = current_stock / avg_weekly_sales
- **Output**: Product, WDD uplift %, current stock, avg sales, WOC, risk level, priority
- **Business Value**: Prevent stockouts by identifying high-risk items needing immediate replenishment

Expected Output Format:
‚Ä¢ Sandwiches: +268% (9,957 ‚Üí 14,571 units, Q4-24 ‚Üí Q1-25)
‚Ä¢ Soda: +214% (1,824 ‚Üí 5,728 units, Q4-24 ‚Üí Q1-25)
‚Ä¢ Bacon: +72% (4,247 ‚Üí 7,312 units, Q4-24 ‚Üí Q1-25)

**"Perishable Products + WDD + Availability Risk" Queries (Tampa 6-Week Analysis - Q13):**
Q: "Which perishable products had the strongest weather-driven demand over the past 6 weeks in Tampa, FL, and do any show low availability risk?"

‚ö†Ô∏è CRITICAL - AVOID RESTRICTIVE FILTERS:
   This query analyzes ALL perishable products in Tampa market.
   DO NOT add specific product filters from entity resolution!
   Let the query find all perishable products with WDD trends.

‚ö†Ô∏è CRITICAL REQUIREMENTS:
   1. **MUST filter perishable products only** - Use ph.category = 'Perishable' filter
   2. **Market filter**: Tampa, FL market ONLY (l.market = 'tampa, fl')
   3. **Time period**: Last 6-7 weeks from CURRENT DATE (2025-11-08) ‚Üí Use: ('2025-09-27', '2025-10-04', '2025-10-11', '2025-10-18', '2025-10-25', '2025-11-01', '2025-11-08')
   4. **WDD calculation**: Use WDD vs LY (metric vs metric_ly) for demand trends
   5. **Weather context**: Include heatwave_flag and cold_spell_flag from weekly_weather
   6. **Availability risk**: Calculate Weeks of Cover (WOC)
   7. **Current inventory date**: 2025-11-08 (stock_at_week_end from batches) - DEMO DATA CURRENT DATE
   8. **Average sales period**: 2025-09-27 to 2025-11-08 (for WOC calculation)
   9. **Weeks of Cover (WOC)**: current_stock / avg_weekly_sales
   10. **Risk levels**: HIGH < 1 week, MEDIUM 1-2 weeks, LOW ‚â• 2 weeks
   11. **Risk priority**: 1=HIGH, 2=MEDIUM, 3=LOW (for sorting)
   12. **Output**: Product, category, WDD trend, weather flags, current stock, avg sales, WOC, risk level
   13. **Sort**: Risk priority ASC (high risk first), then WDD % DESC
   14. **DO NOT add product filters** - Show ALL perishable products with data

‚ö†Ô∏è CRITICAL - DO NOT ADD PRODUCT FILTERS:
   - ‚ùå DO NOT use: WHERE ph.product IN (...)
   - ‚ùå DO NOT use: WHERE p.product_name IN (...)
   - ‚úÖ CORRECT: Filter by ph.category = 'Perishable' ONLY, let query find all matching products
   - **Reason**: Goal is to discover which perishable items have strong WDD + low availability

‚ö†Ô∏è CRITICAL DATES FOR Q13 (CORRECTED FOR DEMO DATA):
   - **DEMO DATA CURRENT DATE**: 2025-11-08 (not 12-27!)
   - Current inventory snapshot: 2025-11-08 (FROM batches WHERE week_end_date = '2025-11-08')
   - Average sales calculation: 2025-09-27 to 2025-11-08 (FROM sales WHERE transaction_date BETWEEN '2025-09-27' AND '2025-11-08')
   - Last 6-7 weeks for WDD: ('2025-09-27', '2025-10-04', '2025-10-11', '2025-10-18', '2025-10-25', '2025-11-01', '2025-11-08')
   - **NOTE**: These dates are DIFFERENT from Q12 dates! Q13 uses PAST data, Q12 uses FUTURE forecast!

‚ö†Ô∏è CRITICAL FORMULAS:
   - **WDD vs LY %**: `ROUND((SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100, 2)`
   - **Weeks of Cover**: `ROUND(current_stock / NULLIF(avg_weekly_sales, 0), 2)`
   - **Risk Level**: `CASE WHEN woc < 1 THEN 'HIGH RISK' WHEN woc < 2 THEN 'MEDIUM RISK' ELSE 'LOW RISK' END`
   - **Risk Priority**: `CASE WHEN woc < 1 THEN 1 WHEN woc < 2 THEN 2 ELSE 3 END`

‚ö†Ô∏è PERISHABLE FILTERING:
   - **Use**: `WHERE ph.category = 'Perishable'` in ALL CTEs
   - This ensures only perishable products are included in analysis
   - Apply to metrics CTE, sales CTE, and batches CTE

Example SQL Template:
```sql
WITH wdd_trends AS (
    -- WDD vs LY trends for last 6 weeks in Tampa for perishable products
    SELECT 
        ph.product,
        ph.category,
        ROUND((SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100, 2) AS wdd_vs_ly_pct,
        COUNT(DISTINCT m.end_date) AS weeks_analyzed,
        MAX(CASE WHEN w.heatwave_flag = true THEN 1 ELSE 0 END) AS had_heatwave,
        MAX(CASE WHEN w.cold_spell_flag = true THEN 1 ELSE 0 END) AS had_cold_spell
    FROM metrics m
    JOIN product_hierarchy ph ON m.product = ph.product
    JOIN location l ON m.location = l.location
    JOIN calendar c ON m.end_date = c.end_date
    LEFT JOIN weekly_weather w ON w.week_end_date = c.end_date AND w.store_id = l.location
    WHERE ph.category = 'Perishable'  -- ONLY perishable products
      AND l.market = 'tampa, fl'  -- Tampa market filter
      AND c.end_date IN ('2025-09-27', '2025-10-04', '2025-10-11', '2025-10-18', '2025-10-25', '2025-11-01', '2025-11-08')  -- Last 6-7 weeks (CORRECTED)
    GROUP BY ph.product, ph.category
    HAVING SUM(m.metric_ly) > 0  -- Must have baseline
),
avg_weekly_sales AS (
    -- Average weekly sales for WOC calculation (11-15 to 12-27)
    SELECT 
        ph.product,
        AVG(s.sales_units) AS avg_weekly_sales,
        SUM(s.sales_units) AS total_sales
    FROM sales s
    JOIN product_hierarchy ph ON s.product_code = ph.product_id
    JOIN calendar c ON s.transaction_date = c.end_date
    JOIN location l ON s.store_code = l.location
    WHERE ph.category = 'Perishable'
      AND l.market = 'tampa, fl'
      AND c.end_date BETWEEN '2025-09-27' AND '2025-11-08'  -- Sales period for WOC (CORRECTED)
    GROUP BY ph.product
    HAVING SUM(s.sales_units) > 0
),
current_stock AS (
    -- Current inventory at 12-27-2025
    SELECT 
        ph.product,
        SUM(b.stock_at_week_end) AS current_stock
    FROM batches b
    JOIN product_hierarchy ph ON b.product_code = ph.product_id
    JOIN location l ON b.store_code = l.location
    WHERE ph.category = 'Perishable'
      AND l.market = 'tampa, fl'
      AND b.week_end_date = '2025-11-08'  -- Current inventory date (CORRECTED)
      AND b.stock_at_week_end > 0
    GROUP BY ph.product
)
SELECT 
    COALESCE(wt.product, aws.product, cs.product) AS product,
    COALESCE(wt.category, 'Perishable') AS category,
    wt.wdd_vs_ly_pct,
    wt.weeks_analyzed,
    CASE WHEN wt.had_heatwave = 1 THEN 'Yes' ELSE 'No' END AS heatwave_present,
    CASE WHEN wt.had_cold_spell = 1 THEN 'Yes' ELSE 'No' END AS cold_spell_present,
    cs.current_stock,
    aws.avg_weekly_sales,
    ROUND(cs.current_stock / NULLIF(aws.avg_weekly_sales, 0), 2) AS weeks_of_cover,
    CASE 
        WHEN cs.current_stock / NULLIF(aws.avg_weekly_sales, 0) < 1 THEN 'HIGH RISK'
        WHEN cs.current_stock / NULLIF(aws.avg_weekly_sales, 0) < 2 THEN 'MEDIUM RISK'
        ELSE 'LOW RISK'
    END AS availability_risk,
    CASE 
        WHEN cs.current_stock / NULLIF(aws.avg_weekly_sales, 0) < 1 THEN 1
        WHEN cs.current_stock / NULLIF(aws.avg_weekly_sales, 0) < 2 THEN 2
        ELSE 3
    END AS risk_priority
FROM wdd_trends wt
LEFT JOIN current_stock cs ON wt.product = cs.product
LEFT JOIN avg_weekly_sales aws ON wt.product = aws.product
WHERE cs.current_stock > 0 AND aws.avg_weekly_sales > 0  -- Only products with inventory and sales
ORDER BY risk_priority ASC, wt.wdd_vs_ly_pct DESC  -- High risk first, then strongest WDD
LIMIT 30;
```

Expected Output Columns:
- product: Product name (e.g., "Salad Kits", "Ice Cream", "Milk")
- category: Should be 'Perishable'
- wdd_vs_ly_pct: WDD vs Last Year % (e.g., 45.23%)
- weeks_analyzed: Number of weeks with data (should be ‚â§6)
- heatwave_present: 'Yes' if any week had heatwave
- cold_spell_present: 'Yes' if any week had cold spell
- current_stock: Units on hand at 12-27-2025
- avg_weekly_sales: Average sales per week (11-15 to 12-27)
- weeks_of_cover: Current stock / avg weekly sales (e.g., 1.5)
- availability_risk: 'HIGH RISK' / 'MEDIUM RISK' / 'LOW RISK'
- risk_priority: 1 / 2 / 3

**Q13 Context - Tampa Perishables + Availability Risk**:
- **Goal**: Identify perishable products with strong WDD and assess stockout risk
- **Location**: Tampa, FL market only
- **Time Period**: Last 6 weeks (11-15 to 12-27-2025)
- **Product Filter**: Perishable category only
- **Weather Context**: Track heatwave and cold spell flags
- **Risk Assessment**: Weeks of Cover (WOC) = current_stock / avg_weekly_sales
- **Business Value**: Prioritize perishable replenishment based on weather-driven demand + risk

**"Heatwave Impact on Perishables + Shrinkage Risk" Queries:**
Q: "Sudden heatwave in San Francisco. How could this impact perishable products and shrinkage risk?"
‚Üí Strategy:
   1. Get WDD forecast for perishable products during heatwave
   2. Also get historical sales during past heatwaves for trend analysis
   3. Filter by category = 'Perishable' for all perishable products
   4. Join with spoilage_report for shrinkage/waste data during hot periods

Example SQL for WDD + Historical Analysis (NO future date filter - use historical heatwave data):
SELECT ph.product,
       ROUND((SUM(m.metric) - SUM(m.metric_nrm)) / NULLIF(SUM(m.metric_nrm), 0) * 100, 2) AS wdd_change_pct,
       SUM(m.metric) as forecasted_demand,
       SUM(m.metric_nrm) as normal_demand
FROM metrics m
JOIN product_hierarchy ph ON m.product = ph.product
JOIN location l ON m.location = l.location
JOIN calendar c ON m.end_date = c.end_date
JOIN weekly_weather w ON w.week_end_date = c.end_date AND w.store_id = l.location
WHERE ph.category = 'Perishable'  -- All perishable products
  AND l.market ILIKE '%san francisco%'
  AND w.heatwave_flag = true
  -- NO future date filter! Use all historical heatwave data to predict impact
GROUP BY ph.product
ORDER BY wdd_change_pct DESC
LIMIT 30;

**"Recommended Ordering Volume" Queries (CRITICAL - MUST USE ACTUAL SALES!):**
===================================================================================
Q: "What is the recommended ordering volume for perishables in Tampa considering weather forecast?"
Q: "What should we order for next week considering weather?"
Q: "Recommend ordering quantity for perishables"

‚ö†Ô∏è CRITICAL FORMULA (from Testing Team) - DO NOT USE metric_nrm OR metric_ly AS BASELINE!
===================================================================================
Adjusted Qty = Last-week ACTUAL sales √ó (1 + WDD %)

This formula MUST use:
1. ACTUAL sales from SALES TABLE (last week) as the baseline - NOT metric_nrm or metric_ly!
2. WDD percentage from metrics table (for next week) as the multiplier
3. NEVER use metric_nrm or metric_ly as the baseline - they are TREND values, not real sales!

WRONG (DO NOT DO THIS - Q5 ERROR):
‚ùå metric_nrm * (1 + wdd_pct) AS recommended_order  -- WRONG! metric_nrm is NOT real sales
‚ùå SUM(m.metric_ly) * (1 + ...) AS recommended_order  -- WRONG! metric_ly is last YEAR metrics, NOT last week sales
‚ùå SUM(m.metric_nrm) * (1 + ...) AS recommended_order  -- WRONG!

CORRECT (ALWAYS DO THIS FOR Q5):
‚úÖ last_week_sales.units * (1 + wdd_pct) AS recommended_order  -- Uses REAL sales from sales table!
‚úÖ FROM sales s WHERE s.transaction_date BETWEEN '2025-11-02' AND '2025-11-08'  -- Last week ACTUAL sales

‚Üí Strategy:
   1. Get ACTUAL SALES from sales table for last week (transaction_date BETWEEN '2025-11-02' AND '2025-11-08')
   2. Get WDD % from metrics table for next week (metric vs metric_nrm)
   3. Calculate: recommended_order = last_week_sales √ó (1 + WDD_pct)

Example SQL (CORRECT approach - MUST use CTE pattern with sales table):
WITH last_week_sales AS (
    -- Get ACTUAL sales from sales table (NOT from metrics!)
    SELECT ph.product, SUM(s.sales_units) as last_week_units
    FROM sales s
    JOIN product_hierarchy ph ON s.product_code = ph.product_id
    JOIN location l ON s.store_code = l.location
    WHERE s.transaction_date BETWEEN '2025-11-02' AND '2025-11-08'  -- Last week
      AND l.market ILIKE '%tampa%'
      AND ph.category = 'Perishable'
    GROUP BY ph.product
),
wdd_forecast AS (
    -- Get WDD percentage from metrics table for next week
    SELECT m.product,
        (SUM(m.metric) - SUM(m.metric_nrm)) / NULLIF(SUM(m.metric_nrm), 0) AS wdd_pct
    FROM metrics m
    JOIN location l ON m.location = l.location
    WHERE m.end_date = '2025-11-15'  -- Next week
      AND l.market ILIKE '%tampa%'
    GROUP BY m.product
)
SELECT 
    lws.product,
    lws.last_week_units AS last_week_sales,
    ROUND(COALESCE(w.wdd_pct, 0) * 100, 2) AS wdd_change_pct,
    ROUND(lws.last_week_units * (1 + COALESCE(w.wdd_pct, 0)), 0) AS recommended_order_qty
FROM last_week_sales lws
LEFT JOIN wdd_forecast w ON lws.product = w.product
ORDER BY wdd_change_pct DESC
LIMIT 30;

14. WHERE vs HAVING CLAUSE (CRITICAL - PREVENTS ERRORS!):
    - WHERE clause: Filters rows BEFORE grouping - NO aggregate functions (SUM, AVG, COUNT, etc.)
    - HAVING clause: Filters groups AFTER grouping - aggregate functions ARE ALLOWED
    
    WRONG Examples (will cause ERROR):
    ‚ùå WHERE SUM(metric) > 100
    ‚ùå WHERE AVG(sales_units) > 50
    ‚ùå WHERE ROUND((SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100, 2) > 0
    
    CORRECT Examples:
    ‚úÖ HAVING SUM(metric) > 100
    ‚úÖ HAVING AVG(sales_units) > 50
    ‚úÖ HAVING ROUND((SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100, 2) > 0
    
    Rule: If filtering by aggregated/calculated values after GROUP BY, use HAVING not WHERE!

15. HANDLING MISSING CATEGORY/DEPARTMENT (CRITICAL!):
    When grouping by category or department, use COALESCE to fallback to product name:
    - SELECT COALESCE(ph.category, ph.product) AS category FROM ...
    - SELECT COALESCE(ph.dept, ph.product) AS department FROM ...
    - GROUP BY COALESCE(ph.category, ph.product)
    This prevents "None (Missing)" - shows product name when category/dept is NULL
    
    Example for category query:
    SELECT COALESCE(ph.category, ph.product) AS category,
           ROUND((SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100, 2) AS wdd_change
    FROM metrics m
    JOIN product_hierarchy ph ON m.product = ph.product
    GROUP BY COALESCE(ph.category, ph.product)
    ORDER BY wdd_change DESC

CRITICAL TABLE NAMES:
- metrics (WDD trends)
- sales (actual transactions)
- batches (inventory batches)
- batch_stock_tracking (stock movements)
- spoilage_report (waste tracking)
- events (festivals, holidays, games)
- weekly_weather
- location
- product_hierarchy
- calendar
- perishable

=== EXAMPLE QUERY PATTERNS ===

-- WDD Analysis: "weather impact on demand"
SELECT ph.product, l.region, ROUND((SUM(m.metric) - SUM(m.metric_nrm)) / NULLIF(SUM(m.metric_nrm), 0) * 100, 2) AS wdd_vs_normal_pct
FROM metrics m
JOIN product_hierarchy ph ON m.product = ph.product
JOIN location l ON m.location = l.location
JOIN calendar c ON m.end_date = c.end_date
WHERE c.end_date BETWEEN '2025-11-08' AND '2025-11-22'
GROUP BY ph.product, l.region ORDER BY wdd_vs_normal_pct DESC LIMIT 30

-- SALES Query: "top selling products"  
SELECT ph.product, SUM(s.sales_units) AS total_units, SUM(s.total_amount) AS total_revenue
FROM sales s
JOIN product_hierarchy ph ON s.product_code = ph.product_id
WHERE s.transaction_date >= '2025-10-01'
GROUP BY ph.product ORDER BY total_revenue DESC LIMIT 30

-- INVENTORY Query: "products low on stock"
SELECT ph.product, l.location, SUM(b.stock_at_week_end) AS current_stock
FROM batches b
JOIN product_hierarchy ph ON b.product_code = ph.product_id
JOIN location l ON b.store_code = l.location
WHERE b.week_end_date = '2025-11-08'
GROUP BY ph.product, l.location HAVING SUM(b.stock_at_week_end) < 100 LIMIT 30

-- EVENTS Query: "football games this week"
SELECT DISTINCT e.event, e.event_type, e.event_date, e.market, e.state
FROM events e
WHERE (e.event ILIKE '%football%' OR e.event_type ILIKE '%sporting%')
  AND e.event_date BETWEEN '2025-11-02' AND '2025-11-08'
ORDER BY e.event_date LIMIT 30

-- SPOILAGE Query: "products with highest waste"
SELECT ph.product, SUM(sr.spoilage_qty) AS total_spoilage, AVG(sr.spoilage_pct) AS avg_spoilage_pct
FROM spoilage_report sr
JOIN product_hierarchy ph ON sr.product_code = ph.product_id
GROUP BY ph.product ORDER BY total_spoilage DESC LIMIT 30

=== CFO-LEVEL BUSINESS FORMULAS (CRITICAL FOR ADVANCED ANALYTICS) ===

These formulas support inventory optimization, stockout risk analysis, financial loss prediction, and sales velocity tracking.
Use these when user asks about: stockout risk, overstock analysis, inventory turnover, sales velocity, demand forecasting, financial loss prediction.

**1. Daily Sales Velocity** (baseline selling speed):
   Formula: Total Sales in Last 28 Days / 28
   Purpose: Establish baseline daily selling rate per product/region
   PostgreSQL Example:
   WITH sales_velocity AS (
     SELECT ph.product, l.region,
            SUM(s.sales_units) / 28.0 AS daily_velocity
     FROM sales s
     JOIN product_hierarchy ph ON s.product_code = ph.product_id
     JOIN location l ON s.store_code = l.location
     WHERE s.transaction_date >= CURRENT_DATE - INTERVAL '28 days'
     GROUP BY ph.product, l.region
   )
   SELECT * FROM sales_velocity ORDER BY daily_velocity DESC;

**2. Adjusted Velocity** (weather-adjusted selling speed):
   Formula: Daily_Sales_Velocity √ó (1 + WDD_Pct_vs_Normal)
   Purpose: Predict future selling speed considering weather impacts
   PostgreSQL Example:
   WITH sales_velocity AS (
     SELECT ph.product, l.region,
            SUM(s.sales_units) / 28.0 AS daily_velocity
     FROM sales s
     JOIN product_hierarchy ph ON s.product_code = ph.product_id
     JOIN location l ON s.store_code = l.location
     WHERE s.transaction_date >= CURRENT_DATE - INTERVAL '28 days'
     GROUP BY ph.product, l.region
   ),
   wdd_forecast AS (
     SELECT m.product, m.location,
            (SUM(m.metric) - SUM(m.metric_nrm)) / NULLIF(SUM(m.metric_nrm), 0) AS wdd_pct
     FROM metrics m
     JOIN calendar c ON m.end_date = c.end_date
     WHERE c.end_date >= '2025-11-15'  -- next week
     GROUP BY m.product, m.location
   )
   SELECT sv.product, sv.region,
          sv.daily_velocity * (1 + COALESCE(w.wdd_pct, 0)) AS adjusted_velocity
   FROM sales_velocity sv
   LEFT JOIN wdd_forecast w ON sv.product = w.product AND sv.region = w.location;

**3. Adjusted Demand** (4-week average adjusted for weather):
   Formula: Avg_4Week_Sales √ó (1 + WDD_Pct_vs_Normal)
   Purpose: Forecast weekly demand considering weather impacts
   PostgreSQL Example:
   WITH avg_sales AS (
     SELECT ph.product, l.region,
            AVG(weekly_sales.units) AS avg_4week_sales
     FROM (
       SELECT s.product_code, s.store_code,
              DATE_TRUNC('week', s.transaction_date) AS week,
              SUM(s.sales_units) AS units
       FROM sales s
       WHERE s.transaction_date >= CURRENT_DATE - INTERVAL '28 days'
       GROUP BY s.product_code, s.store_code, week
     ) weekly_sales
     JOIN product_hierarchy ph ON weekly_sales.product_code = ph.product_id
     JOIN location l ON weekly_sales.store_code = l.location
     GROUP BY ph.product, l.region
   ),
   wdd_forecast AS (
     SELECT m.product, m.location,
            (SUM(m.metric) - SUM(m.metric_nrm)) / NULLIF(SUM(m.metric_nrm), 0) AS wdd_pct
     FROM metrics m
     GROUP BY m.product, m.location
   )
   SELECT a.product, a.region,
          a.avg_4week_sales * (1 + COALESCE(w.wdd_pct, 0)) AS adjusted_demand
   FROM avg_sales a
   LEFT JOIN wdd_forecast w ON a.product = w.product AND a.region = w.location;

**4. Stockout Loss** (potential revenue loss from stockouts):
   Formula: MAX(0, (Adjusted_Velocity √ó 7) - Current_Stock)
   Purpose: Identify products at risk of running out before next delivery
   PostgreSQL Example:
   WITH current_inventory AS (
     SELECT ph.product, l.region,
            SUM(b.stock_at_week_end) AS current_stock
     FROM batches b
     JOIN product_hierarchy ph ON b.product_code = ph.product_id
     JOIN location l ON b.store_code = l.location
     WHERE b.week_end_date = (SELECT MAX(week_end_date) FROM batches)
     GROUP BY ph.product, l.region
   ),
   velocity AS (
     SELECT ph.product, l.region,
            (SUM(s.sales_units) / 28.0) * (1 + COALESCE(
              (SELECT (SUM(m.metric) - SUM(m.metric_nrm)) / NULLIF(SUM(m.metric_nrm), 0)
               FROM metrics m WHERE m.product = ph.product AND m.location = l.location), 0
            )) AS adjusted_velocity
     FROM sales s
     JOIN product_hierarchy ph ON s.product_code = ph.product_id
     JOIN location l ON s.store_code = l.location
     WHERE s.transaction_date >= CURRENT_DATE - INTERVAL '28 days'
     GROUP BY ph.product, l.region
   )
   SELECT i.product, i.region, i.current_stock,
          v.adjusted_velocity * 7 AS weekly_demand,
          GREATEST(0, (v.adjusted_velocity * 7) - i.current_stock) AS stockout_loss_units
   FROM current_inventory i
   JOIN velocity v ON i.product = v.product AND i.region = v.region
   WHERE (v.adjusted_velocity * 7) > i.current_stock
   ORDER BY stockout_loss_units DESC;

**5. Week-on-Week Sales Unit % Change** (regional performance tracking):
   Formula: ((Current_Week_Sales - Previous_Week_Sales) / Previous_Week_Sales) √ó 100
   Purpose: Track sales momentum across regions
   PostgreSQL Example:
   WITH current_week AS (
     SELECT l.region, ph.product, SUM(s.sales_units) AS curr_units
     FROM sales s
     JOIN product_hierarchy ph ON s.product_code = ph.product_id
     JOIN location l ON s.store_code = l.location
     WHERE s.transaction_date BETWEEN '2025-11-02' AND '2025-11-08'
     GROUP BY l.region, ph.product
   ),
   prev_week AS (
     SELECT l.region, ph.product, SUM(s.sales_units) AS prev_units
     FROM sales s
     JOIN product_hierarchy ph ON s.product_code = ph.product_id
     JOIN location l ON s.store_code = l.location
     WHERE s.transaction_date BETWEEN '2025-10-26' AND '2025-11-01'
     GROUP BY l.region, ph.product
   )
   SELECT c.region, c.product,
          ROUND(((c.curr_units - p.prev_units)::NUMERIC / NULLIF(p.prev_units, 0)) * 100, 2) AS wow_change_pct
   FROM current_week c
   JOIN prev_week p ON c.region = p.region AND c.product = p.product
   ORDER BY wow_change_pct DESC;

**6. Sales Uplift vs Historical Average** (demand spike detection):
   Formula: Current_Week_Sales - Four_Week_Avg_Sales
   Purpose: Identify unusual demand spikes requiring inventory adjustment
   PostgreSQL Example:
   WITH current_week AS (
     SELECT ph.product, l.region, SUM(s.sales_units) AS current_units
     FROM sales s
     JOIN product_hierarchy ph ON s.product_code = ph.product_id
     JOIN location l ON s.store_code = l.location
     WHERE s.transaction_date BETWEEN '2025-11-02' AND '2025-11-08'
     GROUP BY ph.product, l.region
   ),
   four_week_avg AS (
     SELECT ph.product, l.region, AVG(weekly.units) AS avg_units
     FROM (
       SELECT s.product_code, s.store_code,
              DATE_TRUNC('week', s.transaction_date) AS week,
              SUM(s.sales_units) AS units
       FROM sales s
       WHERE s.transaction_date >= CURRENT_DATE - INTERVAL '28 days'
       GROUP BY s.product_code, s.store_code, week
     ) weekly
     JOIN product_hierarchy ph ON weekly.product_code = ph.product_id
     JOIN location l ON weekly.store_code = l.location
     GROUP BY ph.product, l.region
   )
   SELECT c.product, c.region,
          c.current_units - a.avg_units AS sales_uplift
   FROM current_week c
   JOIN four_week_avg a ON c.product = a.product AND c.region = a.region
   WHERE c.current_units > a.avg_units
   ORDER BY sales_uplift DESC;

**7. Overstock Percentage by Region** (inventory efficiency):
   Formula: ((Current_Stock - Adjusted_Demand) / Adjusted_Demand) √ó 100
   Purpose: Identify regions with excess inventory requiring markdown/transfers
   PostgreSQL Example:
   WITH current_inventory AS (
     SELECT ph.product, l.region, SUM(b.stock_at_week_end) AS current_stock
     FROM batches b
     JOIN product_hierarchy ph ON b.product_code = ph.product_id
     JOIN location l ON b.store_code = l.location
     WHERE b.week_end_date = (SELECT MAX(week_end_date) FROM batches)
     GROUP BY ph.product, l.region
   ),
   demand_forecast AS (
     SELECT ph.product, l.region,
            AVG(weekly_sales.units) * (1 + COALESCE(
              (SELECT (SUM(m.metric) - SUM(m.metric_nrm)) / NULLIF(SUM(m.metric_nrm), 0)
               FROM metrics m WHERE m.product = ph.product AND m.location = l.location), 0
            )) AS adjusted_demand
     FROM (
       SELECT s.product_code, s.store_code, DATE_TRUNC('week', s.transaction_date) AS week,
              SUM(s.sales_units) AS units
       FROM sales s
       WHERE s.transaction_date >= CURRENT_DATE - INTERVAL '28 days'
       GROUP BY s.product_code, s.store_code, week
     ) weekly_sales
     JOIN product_hierarchy ph ON weekly_sales.product_code = ph.product_id
     JOIN location l ON weekly_sales.store_code = l.location
     GROUP BY ph.product, l.region
   )
   SELECT i.product, i.region,
          ROUND(((i.current_stock - d.adjusted_demand)::NUMERIC / NULLIF(d.adjusted_demand, 0)) * 100, 2) AS overstock_pct
   FROM current_inventory i
   JOIN demand_forecast d ON i.product = d.product AND i.region = d.region
   WHERE i.current_stock > d.adjusted_demand
   ORDER BY overstock_pct DESC;

**8. Shelf-Life Risk** (perishable loss prediction):
   Formulas:
   - Expiring Soon: SUM(stock WHERE days_to_expiry <= shelf_life_days) √ó Avg_Unit_Price
   - Already Expired: SUM(stock WHERE days_to_expiry < 0) √ó Avg_Unit_Price
   Purpose: Quantify financial risk from perishable spoilage
   PostgreSQL Example:
   WITH perishable_inventory AS (
     SELECT ph.product, l.region, b.batch_id,
            p.max_period AS shelf_life_days,
            b.stock_at_week_end,
            EXTRACT(DAY FROM (b.week_end_date - b.received_date)) AS days_since_received,
            p.max_period - EXTRACT(DAY FROM (b.week_end_date - b.received_date)) AS days_to_expiry
     FROM batches b
     JOIN product_hierarchy ph ON b.product_code = ph.product_id
     JOIN location l ON b.store_code = l.location
     JOIN perishable p ON ph.product = p.product
     WHERE b.week_end_date = (SELECT MAX(week_end_date) FROM batches)
       AND p.period_metric = 'Days'
   ),
   avg_prices AS (
     SELECT ph.product, AVG(s.total_amount) AS avg_unit_price
     FROM sales s
     JOIN product_hierarchy ph ON s.product_code = ph.product_id
     GROUP BY ph.product
   )
   SELECT pi.product, pi.region,
          SUM(CASE WHEN pi.days_to_expiry <= 7 AND pi.days_to_expiry > 0
                   THEN pi.stock_at_week_end * ap.avg_unit_price ELSE 0 END) AS expiring_soon_value,
          SUM(CASE WHEN pi.days_to_expiry <= 0
                   THEN pi.stock_at_week_end * ap.avg_unit_price ELSE 0 END) AS expired_value
   FROM perishable_inventory pi
   JOIN avg_prices ap ON pi.product = ap.product
   GROUP BY pi.product, pi.region
   HAVING SUM(CASE WHEN pi.days_to_expiry <= 7 THEN pi.stock_at_week_end ELSE 0 END) > 0
   ORDER BY expiring_soon_value + expired_value DESC;

**9. Stockout Rate** (SKU availability tracking):
   Formula: (SKUs_with_zero_stock / Total_SKUs) √ó 100
   Purpose: Measure inventory availability across product portfolio
   PostgreSQL Example:
   WITH inventory_status AS (
     SELECT ph.product, l.region,
            SUM(b.stock_at_week_end) AS current_stock
     FROM batches b
     JOIN product_hierarchy ph ON b.product_code = ph.product_id
     JOIN location l ON b.store_code = l.location
     WHERE b.week_end_date = (SELECT MAX(week_end_date) FROM batches)
     GROUP BY ph.product, l.region
   )
   SELECT region,
          COUNT(*) AS total_skus,
          SUM(CASE WHEN current_stock = 0 THEN 1 ELSE 0 END) AS stockout_skus,
          ROUND((SUM(CASE WHEN current_stock = 0 THEN 1 ELSE 0 END)::NUMERIC / COUNT(*)) * 100, 2) AS stockout_rate_pct
   FROM inventory_status
   GROUP BY region
   ORDER BY stockout_rate_pct DESC;

**10. Predicted Total Loss** (comprehensive risk assessment):
   Formula: Shelf_Life_Loss + Overstock_Loss
   Purpose: Estimate total financial risk from inventory issues
   PostgreSQL Example:
   -- Combine shelf-life risk (Formula #8) with overstock risk (Formula #7)
   WITH shelf_life_risk AS (
     -- Use Formula #8 logic here
     SELECT product, region, SUM(expiring_value + expired_value) AS shelf_life_loss
     FROM (/* shelf-life calculation */) sub
     GROUP BY product, region
   ),
   overstock_risk AS (
     -- Use Formula #7 logic with markdown assumption (20% loss on excess)
     SELECT product, region,
            GREATEST(0, current_stock - adjusted_demand) * avg_unit_price * 0.2 AS overstock_loss
     FROM (/* overstock calculation */) sub
   )
   SELECT COALESCE(s.product, o.product) AS product,
          COALESCE(s.region, o.region) AS region,
          COALESCE(s.shelf_life_loss, 0) + COALESCE(o.overstock_loss, 0) AS predicted_total_loss
   FROM shelf_life_risk s
   FULL OUTER JOIN overstock_risk o ON s.product = o.product AND s.region = o.region
   ORDER BY predicted_total_loss DESC;

=== NEW CRITICAL FORMULAS (Testing Requirements) ===

**11. Weeks of Cover (WOC)** - Inventory Risk Assessment:
   Formula: Current_Stock / Average_Weekly_Sales
   Purpose: Measure how many weeks current inventory will last
   Risk Levels:
   - HIGH RISK: < 1 week of cover
   - MEDIUM RISK: 1 to < 2 weeks of cover
   - LOW RISK: ‚â• 2 weeks of cover
   
   PostgreSQL Example:
   WITH weekly_sales AS (
     SELECT ph.product, l.location,
            AVG(week_sales.units) AS avg_weekly_sales
     FROM (
       SELECT s.product_code, s.store_code,
              DATE_TRUNC('week', s.transaction_date) AS week,
              SUM(s.sales_units) AS units
       FROM sales s
       WHERE s.transaction_date BETWEEN '2025-10-12' AND '2025-11-08'  -- Last 4 weeks
       GROUP BY s.product_code, s.store_code, week
     ) week_sales
     JOIN product_hierarchy ph ON week_sales.product_code = ph.product_id
     JOIN location l ON week_sales.store_code = l.location
     GROUP BY ph.product, l.location
   ),
   current_inventory AS (
     SELECT ph.product, l.location,
            SUM(b.stock_at_week_end) AS current_stock
     FROM batches b
     JOIN product_hierarchy ph ON b.product_code = ph.product_id
     JOIN location l ON b.store_code = l.location
     WHERE b.week_end_date = '2025-12-27'  -- Current week end
     GROUP BY ph.product, l.location
   )
   SELECT ci.product, ci.location,
          ci.current_stock,
          ws.avg_weekly_sales,
          ROUND(ci.current_stock / NULLIF(ws.avg_weekly_sales, 0), 2) AS weeks_of_cover,
          CASE 
            WHEN ci.current_stock / NULLIF(ws.avg_weekly_sales, 0) < 1 THEN 'HIGH RISK'
            WHEN ci.current_stock / NULLIF(ws.avg_weekly_sales, 0) < 2 THEN 'MEDIUM RISK'
            ELSE 'LOW RISK'
          END AS risk_level,
          CASE 
            WHEN ci.current_stock / NULLIF(ws.avg_weekly_sales, 0) < 1 THEN 1
            WHEN ci.current_stock / NULLIF(ws.avg_weekly_sales, 0) < 2 THEN 2
            ELSE 3
          END AS risk_priority
   FROM current_inventory ci
   JOIN weekly_sales ws ON ci.product = ws.product AND ci.location = ws.location
   WHERE ci.current_stock > 0
   ORDER BY weeks_of_cover ASC;

**12. Weekend Filtering for Ideal Beach Weather**:
   Purpose: Filter for Saturday week_end_date with ideal beach conditions
   Conditions:
   - Temperature: 80-95¬∞F (use tmax, tmin from weekly_weather)
   - Rain: ‚â§ 0.1 inches (use percin column)
   - No adverse weather flags: heavy_rain = false, cold_spell = false, snow = false
   - Day of week: Saturday (week_end_date)
   
   PostgreSQL Example:
   SELECT DISTINCT c.end_date
   FROM calendar c
   JOIN weekly_weather ww ON c.end_date = ww.week_end_date
   WHERE EXTRACT(DOW FROM c.end_date) = 6  -- Saturday = 6
     AND ww.tmax_f BETWEEN 80 AND 95
     AND ww.tmin_f >= 70  -- Comfortable minimum
     AND ww.precip_in <= 0.1  -- Minimal rain (inches)
     AND ww.heatwave_flag = false
     AND ww.cold_spell_flag = false
     AND ww.heavy_rain_flag = false
     AND ww.snow_flag = false;

‚ö†Ô∏è CRITICAL: Beach Weather Food Diversification Query Pattern (e.g., "How should food options be diversified for peak weekend sales driven by ideal beach weather in Miami?")

WHEN USER ASKS: "diversify", "diversification", "beach weather", "ideal beach", "peak weekend sales"

MUST DO:
1. **Use METRICS table** (NOT sales table!) for WDD vs LY analysis
2. **Calculate WDD vs LY percentage**: (SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100
3. **Filter 2 years historical data**: c.end_date BETWEEN '2023-11-08' AND '2025-11-08'
4. **Filter Saturday weekends**: EXTRACT(DOW FROM c.end_date) = 6
5. **Apply ideal beach weather conditions**:
   - w.tmax_f BETWEEN 80 AND 95  -- Fahrenheit
   - w.tmin_f >= 70
   - w.precip_in <= 0.1  -- inches
   - w.heatwave_flag = false AND w.cold_spell_flag = false
   - w.heavy_rain_flag = false AND w.snow_flag = false
6. **Join pattern**: metrics ‚Üí product_hierarchy ‚Üí location ‚Üí calendar ‚Üí weekly_weather
7. **Group by**: ph.product, ph.category
8. **Order by**: wdd_vs_ly_pct DESC

CORRECT QUERY TEMPLATE:
```sql
SELECT 
    ph.product,
    COALESCE(ph.category, 'General') AS category,
    COALESCE(ph.dept, 'General') AS dept,
    ROUND((SUM(m.metric) - SUM(m.metric_ly)) / NULLIF(SUM(m.metric_ly), 0) * 100, 2) AS wdd_vs_ly_pct,
    COUNT(DISTINCT m.end_date) AS num_ideal_weekends
FROM metrics m
JOIN product_hierarchy ph ON m.product = ph.product
JOIN location l ON m.location = l.location
JOIN calendar c ON m.end_date = c.end_date
JOIN weekly_weather w ON w.week_end_date = c.end_date AND w.store_id = l.location
WHERE l.market ILIKE '%miami%'
  AND EXTRACT(DOW FROM c.end_date) = 6
  AND c.end_date BETWEEN '2023-11-08' AND '2025-11-08'
  AND w.tmax_f BETWEEN 80 AND 95
  AND w.tmin_f >= 70
  AND w.precip_in <= 0.1
  AND w.heatwave_flag = false AND w.cold_spell_flag = false
  AND w.heavy_rain_flag = false AND w.snow_flag = false
GROUP BY ph.product, ph.category, ph.dept
HAVING SUM(m.metric_ly) > 0
ORDER BY wdd_vs_ly_pct DESC
LIMIT 30;
```

‚ùå WRONG PATTERNS TO AVOID:
- Using sales table instead of metrics table
- Missing date range filter (2 years historical)
- Using current week data only
- Not filtering for Saturday (DOW = 6)
- Missing weather condition filters
- Using revenue calculations instead of WDD percentage

**13. Quarter-on-Quarter (QoQ) Sales Comparison**:
   Purpose: Compare sales performance across quarters to identify spikes
   Formula: ((Current_Quarter_Sales - Previous_Quarter_Sales) / Previous_Quarter_Sales) √ó 100
   
   PostgreSQL Example:
   WITH quarterly_sales AS (
     SELECT ph.product, l.market,
            c.quarter, c.year,
            SUM(s.sales_units) AS total_units
     FROM sales s
     JOIN product_hierarchy ph ON s.product_code = ph.product_id
     JOIN location l ON s.store_code = l.location
     JOIN calendar c ON DATE_TRUNC('week', s.transaction_date)::date = c.end_date
     WHERE c.end_date BETWEEN '2023-07-01' AND '2025-11-08'  -- Last 8 quarters
     GROUP BY ph.product, l.market, c.quarter, c.year
   ),
   lagged_sales AS (
     SELECT *,
            LAG(total_units) OVER (PARTITION BY product, market ORDER BY year, quarter) AS prev_quarter_units
     FROM quarterly_sales
   )
   SELECT product, market, quarter, year, total_units, prev_quarter_units,
          ROUND(((total_units - prev_quarter_units)::NUMERIC / NULLIF(prev_quarter_units, 0)) * 100, 2) AS qoq_change_pct
   FROM lagged_sales
   WHERE prev_quarter_units IS NOT NULL
   ORDER BY qoq_change_pct DESC;

**14. Event Proximity Checking**:
   Purpose: Check if events occurred near stores during specific weeks
   Logic: Find events within same market/state during the week
   
   PostgreSQL Example:
   SELECT DISTINCT c.end_date, l.market, l.state,
          COUNT(e.event) AS event_count,
          STRING_AGG(e.event, ', ') AS events_nearby
   FROM calendar c
   CROSS JOIN location l
   LEFT JOIN events e ON l.market = e.market 
     AND e.event_date BETWEEN c.end_date - INTERVAL '7 days' AND c.end_date
   WHERE c.end_date IN ('2025-01-11', '2025-01-18', '2025-02-15')  -- Specific weeks
     AND l.market = 'columbia, sc'
   GROUP BY c.end_date, l.market, l.state
   HAVING COUNT(e.event) = 0;  -- Only weeks with NO events

**15. Improved Shelf Life Risk with Daily Sales Velocity**:
   Purpose: Calculate precise expiry risk using daily selling rate
   Formula:
   - Days to Expiry > 0: Shelf_Life_Loss = (Current_Stock - (Daily_Sales_Velocity √ó Days_To_Expiry)) √ó Unit_Price
   - Days to Expiry ‚â§ 0: Shelf_Life_Loss = Current_Stock √ó Unit_Price
   
   PostgreSQL Example:
   WITH daily_velocity AS (
     SELECT ph.product, l.location,
            SUM(s.sales_units) / 28.0 AS daily_sales_velocity
     FROM sales s
     JOIN product_hierarchy ph ON s.product_code = ph.product_id
     JOIN location l ON s.store_code = l.location
     WHERE s.transaction_date >= CURRENT_DATE - INTERVAL '28 days'
     GROUP BY ph.product, l.location
   ),
   perishable_stock AS (
     SELECT ph.product, l.location, b.batch_id,
            b.stock_at_week_end AS current_stock,
            p.max_period - EXTRACT(DAY FROM ('2025-11-08'::date - b.received_date)) AS days_to_expiry,
            (SELECT AVG(total_amount) FROM sales WHERE product_code = b.product_code) AS unit_price
     FROM batches b
     JOIN product_hierarchy ph ON b.product_code = ph.product_id
     JOIN location l ON b.store_code = l.location
     JOIN perishable p ON ph.product = p.product
     WHERE b.week_end_date = '2025-11-08'
   )
   SELECT ps.product, ps.location,
          ps.current_stock,
          dv.daily_sales_velocity,
          ps.days_to_expiry,
          CASE 
            WHEN ps.days_to_expiry > 0 THEN 
              GREATEST(0, ps.current_stock - (dv.daily_sales_velocity * ps.days_to_expiry)) * ps.unit_price
            ELSE 
              ps.current_stock * ps.unit_price
          END AS shelf_life_loss
   FROM perishable_stock ps
   JOIN daily_velocity dv ON ps.product = dv.product AND ps.location = dv.location
   WHERE ps.days_to_expiry <= 7
   ORDER BY shelf_life_loss DESC;

=== WHEN TO USE CFO FORMULAS ===
- "stockout risk" / "running out" ‚Üí Use Formula #4 (Stockout Loss)
- "inventory optimization" / "overstock" ‚Üí Use Formula #7 (Overstock %)
- "sales velocity" / "selling speed" ‚Üí Use Formula #1 (Daily Sales Velocity) or #2 (Adjusted Velocity)
- "demand forecast" / "order planning" ‚Üí Use Formula #3 (Adjusted Demand)
- "week-over-week" / "regional performance" ‚Üí Use Formula #5 (WoW Change)
- "demand spike" / "unusual sales" ‚Üí Use Formula #6 (Sales Uplift)
- "perishable risk" / "expiring stock" ‚Üí Use Formula #8 or #15 (Shelf-Life Risk)
- "SKU availability" / "out of stock rate" ‚Üí Use Formula #9 (Stockout Rate)
- "financial loss" / "total risk" ‚Üí Use Formula #10 (Predicted Total Loss)
- "weeks of cover" / "inventory duration" / "how long will stock last" ‚Üí Use Formula #11 (Weeks of Cover)
- "weekend sales" / "beach weather" ‚Üí Use Formula #12 (Weekend Filtering)
- "quarter comparison" / "quarterly growth" ‚Üí Use Formula #13 (QoQ Comparison)
- "event proximity" / "no events nearby" ‚Üí Use Formula #14 (Event Proximity)
- "prevent waste" / "ordering advice" / "adjust ordering" ‚Üí MUST USE BOTH:
  1. Formula #3 (Adjusted Demand) for recommended order quantity
  2. Formula #15 (Shelf Life Risk) for waste prevention analysis
  3. Include daily_sales_velocity, current_stock, days_to_expiry, potential_waste_units

CRITICAL FOR Q3-TYPE QUERIES (prevent waste + adjust ordering):
When query asks about "prevent waste", "adjust ordering", or "ordering advice" for perishables:
- ALWAYS include shelf life risk calculation (Formula #15)
- ALWAYS calculate daily sales velocity
- ALWAYS check current stock levels from batches
- ALWAYS calculate days to expiry
- ALWAYS provide potential waste units
- Answer MUST address BOTH: (1) demand change % AND (2) specific waste prevention advice

Generate the PostgreSQL SELECT query:
""")
        
        return "\n".join(prompt_parts)
    
    def format_context_summary(self, context: Dict[str, Any]) -> str:
        """Format context as human-readable summary for debugging/logging"""
        summary = []
        
        products = context.get("products", {})
        if products.get("resolved"):
            summary.append(f"‚úì {len(products['resolved'])} products identified")
        if products.get("expanded"):
            summary.append(f"‚úì Expanded to {len(products['expanded'])} related products")
        
        locations = context.get("locations", {})
        if locations.get("resolved"):
            summary.append(f"‚úì {len(locations['resolved'])} locations identified")
        if locations.get("expanded"):
            summary.append(f"‚úì Expanded to {len(locations['expanded'])} stores")
        
        dates = context.get("dates", {})
        if dates.get("date_range"):
            summary.append(f"‚úì Date range: {dates['date_range'][0]} to {dates['date_range'][1]}")
        
        events = context.get("events", {})
        if events.get("resolved"):
            summary.append(f"‚úì {len(events['resolved'])} events found")
        
        return " | ".join(summary) if summary else "No specific context resolved"


# Global instance
context_resolver = ContextResolver()
